{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29cf3d3e",
   "metadata": {},
   "source": [
    "# Implementinf a vanilla MAML on MNIST data set \n",
    "- this is the FOMAML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89e8848c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ef583a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9e28e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:mps\n"
     ]
    }
   ],
   "source": [
    "# -- Hyper parameters --\n",
    "n_way = 5 # the total number of classes in which i want to classify\n",
    "k_shot = 1 # the total number of support examples per class\n",
    "q_queries = 16 # the total number of query examples per class\n",
    "inner_lr = 0.02 # the learning rate for the inner loop (task specific adaptation)\n",
    "meta_lr = 0.001 # learning rate for the outer loop (meta-optimisation)\n",
    "meta_batch_size = 32 # number of tasks per meta-batch\n",
    "training_step = 1000 # number of meta updates to perform\n",
    "inner_steps = 5 # the number of gradient steps\n",
    "\n",
    "# -- Device Setup --\n",
    "device = torch.device(\"mps\")\n",
    "print(f'Using device:{device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff7a6fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:03<00:00, 2.99MB/s]\n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 140kB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:04<00:00, 368kB/s]\n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 4.98MB/s]\n"
     ]
    }
   ],
   "source": [
    "# -- Data Loading --\n",
    "# Seperate the dataset per class so that sampling becomes easier\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "mnist_train = datasets.MNIST('data', train=True, transform=transform, download=True)\n",
    "# Grouping the data by labels\n",
    "train_data_byclass = {i:[] for i in range(10)}\n",
    "for x,y in mnist_train:\n",
    "    train_data_byclass[y].append(x)\n",
    "for i in range(10):\n",
    "    # stack the list of image into a single tensor for each class\n",
    "    train_data_byclass[i] = torch.stack(train_data_byclass[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06e75f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Task Sampling --\n",
    "''' One of the most important step. We need a function that can\n",
    "sample a batch from the dataset where each has S, Q'''\n",
    "def sample_task_batch(data_by_class, batch_size, n_way, k_shot, q_queries):\n",
    "    support_x_batch, support_y_batch = [], []\n",
    "    query_x_batch, query_y_batch = [], []\n",
    "    for _ in range(batch_size):\n",
    "        # 1. Select the N classes for the task\n",
    "        selected_class = np.random.choice(10, n_way, replace=False)\n",
    "        support_x, support_y = [], []\n",
    "        query_x, query_y = [], []\n",
    "        for i, idx in enumerate(data_by_class):\n",
    "            # 2. sample K-shot support and q_queries for each class\n",
    "            class_data = data_by_class[idx]\n",
    "            num_samples= class_data.shape[0] # checks how many samples are there in the specific class i\n",
    "\n",
    "            if num_samples<k_shot+q_queries:\n",
    "                raise ValueError(f\"Not enough samples for class {idx}\")\n",
    "            # sample the indices without replacement\n",
    "            shuffled_indices = torch.randperm(num_samples)\n",
    "            support_indices = shuffled_indices[:k_shot]\n",
    "            query_indices = shuffled_indices[k_shot:k_shot+q_queries]\n",
    "            support_x.append(class_data[support_indices])\n",
    "            # for the samples for k_shot samples, create a tensor of size=kshot, fill value = class (here it becomes 'i')\n",
    "            support_y.append(torch.full((k_shot,), i, dtype=torch.long))\n",
    "            query_x.append(class_data[query_indices])\n",
    "            query_y.append(torch.full((q_queries,), i, dtype=torch.long))\n",
    "\n",
    "        # append to the batch\n",
    "        support_x_batch.append(torch.cat(support_x))\n",
    "        support_y_batch.append(torch.cat(support_y))\n",
    "        query_x_batch.append(torch.cat(query_x))\n",
    "        query_y_batch.append(torch.cat(query_y))\n",
    "    # stack the list of tensor to create a single batch tensor\n",
    "    return (torch.stack(support_x_batch), torch.stack(support_y_batch),\n",
    "            torch.stack(query_x_batch), torch.stack(query_y_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c4a1952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Base learner Model: A CNN --\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, in_channel=1, n_way=5):\n",
    "        super(CNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channel, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "            )\n",
    "        self.classifier = nn.Linear(64*7*7, n_way)\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0aebe6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_model = CNN(in_channel=1, n_way=5).to(device)\n",
    "meta_optimizer = optim.Adam(meta_model.parameters(), lr= meta_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "380a2537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of CNN(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Linear(in_features=3136, out_features=5, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_model.parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db67c39",
   "metadata": {},
   "source": [
    "## The following is the first order approximation for the maml code\n",
    "- Even in the original paper the authors have stated that the use of FOMAML gave close to similar results to MAML \n",
    "- The reason they specified was with a second order derivative - it seemed like a lot of gradients were being reduced to zero also adding to the vanishing gradients\n",
    "- and when compared against the FOMAML, the revelation was that the post-updated weight gradients had the most contribution to selection of good initialization rather than the gradient of the gradient\n",
    "- and the backpropogation is done on the pre-updated weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "98c149a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the FOMAML Training...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     28\u001b[39m average_meta_ls = total_meta_ls/ meta_batch_size\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Propogate the gradients from meta-loss to the original meta model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[43maverage_meta_ls\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m meta_optimizer.step()\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (step + \u001b[32m1\u001b[39m) % \u001b[32m100\u001b[39m ==\u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GITHUB/MAML/maml/lib/python3.13/site-packages/torch/_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GITHUB/MAML/maml/lib/python3.13/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GITHUB/MAML/maml/lib/python3.13/site-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(\"Starting the FOMAML Training...\")\n",
    "# -- Outer Loop: Meta- training --\n",
    "for step in range(training_step):\n",
    "    # 1. Sample a batch of tasks\n",
    "    support_x, support_y, query_x, query_y = sample_task_batch(train_data_byclass, batch_size=meta_batch_size, n_way=n_way, k_shot=k_shot, q_queries=q_queries)\n",
    "    meta_optimizer.zero_grad()\n",
    "    total_meta_ls = 0.0\n",
    "    # -- Inner Loop --\n",
    "    for i in range(meta_batch_size):\n",
    "        # We create a temp model which thus breaks the main compuation graph\n",
    "        fast_model = copy.deepcopy(meta_model)\n",
    "        fast_optim = optim.SGD(fast_model.parameters(), lr = inner_lr)\n",
    "        # For each task\n",
    "        sx, sy = support_x[i].to(device), support_y[i].to(device)\n",
    "        qx, qy = query_x[i].to(device), query_y[i].to(device)\n",
    "        # 2. Ineer loop: train the fast_model on support set\n",
    "        for _ in range(inner_steps):\n",
    "            fast_optim.zero_grad()\n",
    "            logits = fast_model(sx)\n",
    "            loss = F.cross_entropy(logits, sy)\n",
    "            loss.backward()\n",
    "            fast_optim.step()\n",
    "        # Calculate the meta loss on the query using fast_model\n",
    "        query_logits = fast_model(qx)\n",
    "        task_meta_loss = F.cross_entropy(query_logits, qy)\n",
    "        total_meta_ls+= task_meta_loss\n",
    "    # 4. Outer loop update: we change the meta model weights\n",
    "    average_meta_ls = total_meta_ls/ meta_batch_size\n",
    "    # Propogate the gradients from meta-loss to the original meta model\n",
    "    average_meta_ls.backward()\n",
    "    meta_optimizer.step()\n",
    "    if (step + 1) % 100 ==0:\n",
    "        print(f\"Step {step+1}/{training_step}, Meta Loss: {average_meta_ls.item():.4f}\")\n",
    "print(\"FOMAML training done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ac3d41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the FOMAML Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88f2228e24f04bb993621bb911b79165",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Meta-training steps:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/1000, Meta Loss: 0.5109, Time: 1.70s\n",
      "Step 2/1000, Meta Loss: 0.5432, Time: 0.73s\n",
      "Step 3/1000, Meta Loss: 0.4943, Time: 0.65s\n",
      "Step 4/1000, Meta Loss: 0.5444, Time: 0.60s\n",
      "Step 5/1000, Meta Loss: 0.4903, Time: 0.58s\n",
      "Step 6/1000, Meta Loss: 0.5445, Time: 0.58s\n",
      "Step 7/1000, Meta Loss: 0.5277, Time: 0.58s\n",
      "Step 8/1000, Meta Loss: 0.5163, Time: 0.60s\n",
      "Step 9/1000, Meta Loss: 0.5079, Time: 0.59s\n",
      "Step 10/1000, Meta Loss: 0.5033, Time: 0.59s\n",
      "Step 11/1000, Meta Loss: 0.5219, Time: 0.57s\n",
      "Step 12/1000, Meta Loss: 0.5247, Time: 0.57s\n",
      "Step 13/1000, Meta Loss: 0.5310, Time: 0.63s\n",
      "Step 14/1000, Meta Loss: 0.5641, Time: 0.59s\n",
      "Step 15/1000, Meta Loss: 0.5304, Time: 0.58s\n",
      "Step 16/1000, Meta Loss: 0.5284, Time: 0.57s\n",
      "Step 17/1000, Meta Loss: 0.5095, Time: 0.56s\n",
      "Step 18/1000, Meta Loss: 0.5105, Time: 0.68s\n",
      "Step 19/1000, Meta Loss: 0.5271, Time: 0.59s\n",
      "Step 20/1000, Meta Loss: 0.5039, Time: 0.69s\n",
      "Step 21/1000, Meta Loss: 0.5569, Time: 0.63s\n",
      "Step 22/1000, Meta Loss: 0.5360, Time: 0.61s\n",
      "Step 23/1000, Meta Loss: 0.5147, Time: 0.58s\n",
      "Step 24/1000, Meta Loss: 0.5103, Time: 0.61s\n",
      "Step 25/1000, Meta Loss: 0.5428, Time: 0.58s\n",
      "Step 26/1000, Meta Loss: 0.5228, Time: 0.63s\n",
      "Step 27/1000, Meta Loss: 0.5230, Time: 0.57s\n",
      "Step 28/1000, Meta Loss: 0.4910, Time: 0.57s\n",
      "Step 29/1000, Meta Loss: 0.5321, Time: 0.63s\n",
      "Step 30/1000, Meta Loss: 0.5491, Time: 0.66s\n",
      "Step 31/1000, Meta Loss: 0.5266, Time: 0.58s\n",
      "Step 32/1000, Meta Loss: 0.5054, Time: 0.57s\n",
      "Step 33/1000, Meta Loss: 0.5027, Time: 0.58s\n",
      "Step 34/1000, Meta Loss: 0.5211, Time: 0.71s\n",
      "Step 35/1000, Meta Loss: 0.5282, Time: 0.61s\n",
      "Step 36/1000, Meta Loss: 0.5010, Time: 0.58s\n",
      "Step 37/1000, Meta Loss: 0.4998, Time: 0.59s\n",
      "Step 38/1000, Meta Loss: 0.5105, Time: 0.58s\n",
      "Step 39/1000, Meta Loss: 0.5306, Time: 0.57s\n",
      "Step 40/1000, Meta Loss: 0.5205, Time: 0.68s\n",
      "Step 41/1000, Meta Loss: 0.4940, Time: 0.60s\n",
      "Step 42/1000, Meta Loss: 0.5036, Time: 0.58s\n",
      "Step 43/1000, Meta Loss: 0.5227, Time: 0.58s\n",
      "Step 44/1000, Meta Loss: 0.5331, Time: 0.57s\n",
      "Step 45/1000, Meta Loss: 0.5018, Time: 0.58s\n",
      "Step 46/1000, Meta Loss: 0.5109, Time: 0.57s\n",
      "Step 47/1000, Meta Loss: 0.5386, Time: 0.58s\n",
      "Step 48/1000, Meta Loss: 0.5037, Time: 0.64s\n",
      "Step 49/1000, Meta Loss: 0.5436, Time: 0.60s\n",
      "Step 50/1000, Meta Loss: 0.5545, Time: 0.72s\n",
      "Step 51/1000, Meta Loss: 0.4987, Time: 0.71s\n",
      "Step 52/1000, Meta Loss: 0.5140, Time: 0.64s\n",
      "Step 53/1000, Meta Loss: 0.5000, Time: 0.60s\n",
      "Step 54/1000, Meta Loss: 0.5014, Time: 0.58s\n",
      "Step 55/1000, Meta Loss: 0.5431, Time: 0.57s\n",
      "Step 56/1000, Meta Loss: 0.5386, Time: 0.57s\n",
      "Step 57/1000, Meta Loss: 0.5172, Time: 0.57s\n",
      "Step 58/1000, Meta Loss: 0.5269, Time: 0.56s\n",
      "Step 59/1000, Meta Loss: 0.5150, Time: 0.56s\n",
      "Step 60/1000, Meta Loss: 0.5080, Time: 0.56s\n",
      "Step 61/1000, Meta Loss: 0.5288, Time: 0.56s\n",
      "Step 62/1000, Meta Loss: 0.4826, Time: 0.56s\n",
      "Step 63/1000, Meta Loss: 0.5257, Time: 0.56s\n",
      "Step 64/1000, Meta Loss: 0.4966, Time: 0.64s\n",
      "Step 65/1000, Meta Loss: 0.4870, Time: 0.60s\n",
      "Step 66/1000, Meta Loss: 0.5223, Time: 0.67s\n",
      "Step 67/1000, Meta Loss: 0.5412, Time: 0.59s\n",
      "Step 68/1000, Meta Loss: 0.5184, Time: 0.59s\n",
      "Step 69/1000, Meta Loss: 0.5590, Time: 0.58s\n",
      "Step 70/1000, Meta Loss: 0.5377, Time: 0.58s\n",
      "Step 71/1000, Meta Loss: 0.5135, Time: 0.58s\n",
      "Step 72/1000, Meta Loss: 0.5257, Time: 0.57s\n",
      "Step 73/1000, Meta Loss: 0.5400, Time: 0.57s\n",
      "Step 74/1000, Meta Loss: 0.5275, Time: 0.57s\n",
      "Step 75/1000, Meta Loss: 0.4985, Time: 0.57s\n",
      "Step 76/1000, Meta Loss: 0.5189, Time: 0.56s\n",
      "Step 77/1000, Meta Loss: 0.5244, Time: 0.56s\n",
      "Step 78/1000, Meta Loss: 0.4915, Time: 0.56s\n",
      "Step 79/1000, Meta Loss: 0.5059, Time: 0.57s\n",
      "Step 80/1000, Meta Loss: 0.5112, Time: 0.56s\n",
      "Step 81/1000, Meta Loss: 0.5491, Time: 0.57s\n",
      "Step 82/1000, Meta Loss: 0.5663, Time: 0.57s\n",
      "Step 83/1000, Meta Loss: 0.5015, Time: 0.57s\n",
      "Step 84/1000, Meta Loss: 0.5778, Time: 0.57s\n",
      "Step 85/1000, Meta Loss: 0.5443, Time: 0.56s\n",
      "Step 86/1000, Meta Loss: 0.5478, Time: 0.58s\n",
      "Step 87/1000, Meta Loss: 0.5096, Time: 0.57s\n",
      "Step 88/1000, Meta Loss: 0.5429, Time: 0.56s\n",
      "Step 89/1000, Meta Loss: 0.4985, Time: 0.58s\n",
      "Step 90/1000, Meta Loss: 0.5172, Time: 0.64s\n",
      "Step 91/1000, Meta Loss: 0.5001, Time: 0.60s\n",
      "Step 92/1000, Meta Loss: 0.5443, Time: 0.55s\n",
      "Step 93/1000, Meta Loss: 0.5019, Time: 0.57s\n",
      "Step 94/1000, Meta Loss: 0.5106, Time: 0.65s\n",
      "Step 95/1000, Meta Loss: 0.5018, Time: 0.60s\n",
      "Step 96/1000, Meta Loss: 0.5216, Time: 0.59s\n",
      "Step 97/1000, Meta Loss: 0.4906, Time: 0.56s\n",
      "Step 98/1000, Meta Loss: 0.5378, Time: 0.56s\n",
      "Step 99/1000, Meta Loss: 0.5197, Time: 0.56s\n",
      "Step 100/1000, Meta Loss: 0.5272, Time: 0.57s\n",
      "Step 101/1000, Meta Loss: 0.4864, Time: 0.56s\n",
      "Step 102/1000, Meta Loss: 0.5445, Time: 0.56s\n",
      "Step 103/1000, Meta Loss: 0.5124, Time: 0.56s\n",
      "Step 104/1000, Meta Loss: 0.5281, Time: 0.56s\n",
      "Step 105/1000, Meta Loss: 0.5468, Time: 0.56s\n",
      "Step 106/1000, Meta Loss: 0.4910, Time: 0.57s\n",
      "Step 107/1000, Meta Loss: 0.4936, Time: 0.58s\n",
      "Step 108/1000, Meta Loss: 0.5394, Time: 0.58s\n",
      "Step 109/1000, Meta Loss: 0.5109, Time: 0.58s\n",
      "Step 110/1000, Meta Loss: 0.5035, Time: 0.57s\n",
      "Step 111/1000, Meta Loss: 0.5458, Time: 0.57s\n",
      "Step 112/1000, Meta Loss: 0.5183, Time: 0.57s\n",
      "Step 113/1000, Meta Loss: 0.5277, Time: 0.64s\n",
      "Step 114/1000, Meta Loss: 0.5100, Time: 0.59s\n",
      "Step 115/1000, Meta Loss: 0.5055, Time: 0.66s\n",
      "Step 116/1000, Meta Loss: 0.5327, Time: 0.58s\n",
      "Step 117/1000, Meta Loss: 0.5233, Time: 0.60s\n",
      "Step 118/1000, Meta Loss: 0.5466, Time: 0.58s\n",
      "Step 119/1000, Meta Loss: 0.5324, Time: 0.57s\n",
      "Step 120/1000, Meta Loss: 0.5366, Time: 0.56s\n",
      "Step 121/1000, Meta Loss: 0.5200, Time: 0.57s\n",
      "Step 122/1000, Meta Loss: 0.5027, Time: 0.57s\n",
      "Step 123/1000, Meta Loss: 0.4942, Time: 0.57s\n",
      "Step 124/1000, Meta Loss: 0.5264, Time: 0.58s\n",
      "Step 125/1000, Meta Loss: 0.5083, Time: 0.57s\n",
      "Step 126/1000, Meta Loss: 0.5183, Time: 0.58s\n",
      "Step 127/1000, Meta Loss: 0.5589, Time: 0.57s\n",
      "Step 128/1000, Meta Loss: 0.5269, Time: 0.56s\n",
      "Step 129/1000, Meta Loss: 0.4927, Time: 0.56s\n",
      "Step 130/1000, Meta Loss: 0.5258, Time: 0.58s\n",
      "Step 131/1000, Meta Loss: 0.5039, Time: 0.57s\n",
      "Step 132/1000, Meta Loss: 0.5185, Time: 0.59s\n",
      "Step 133/1000, Meta Loss: 0.5113, Time: 0.64s\n",
      "Step 134/1000, Meta Loss: 0.5328, Time: 0.60s\n",
      "Step 135/1000, Meta Loss: 0.4973, Time: 0.58s\n",
      "Step 136/1000, Meta Loss: 0.5106, Time: 0.57s\n",
      "Step 137/1000, Meta Loss: 0.5059, Time: 0.57s\n",
      "Step 138/1000, Meta Loss: 0.5334, Time: 0.57s\n",
      "Step 139/1000, Meta Loss: 0.5146, Time: 0.58s\n",
      "Step 140/1000, Meta Loss: 0.4982, Time: 0.57s\n",
      "Step 141/1000, Meta Loss: 0.5016, Time: 0.57s\n",
      "Step 142/1000, Meta Loss: 0.5113, Time: 0.57s\n",
      "Step 143/1000, Meta Loss: 0.4938, Time: 0.57s\n",
      "Step 144/1000, Meta Loss: 0.5083, Time: 0.57s\n",
      "Step 145/1000, Meta Loss: 0.5054, Time: 0.58s\n",
      "Step 146/1000, Meta Loss: 0.5285, Time: 0.57s\n",
      "Step 147/1000, Meta Loss: 0.5291, Time: 0.57s\n",
      "Step 148/1000, Meta Loss: 0.5204, Time: 0.57s\n",
      "Step 149/1000, Meta Loss: 0.5256, Time: 0.57s\n",
      "Step 150/1000, Meta Loss: 0.4934, Time: 0.58s\n",
      "Step 151/1000, Meta Loss: 0.4987, Time: 0.57s\n",
      "Step 152/1000, Meta Loss: 0.5297, Time: 0.57s\n",
      "Step 153/1000, Meta Loss: 0.5198, Time: 0.58s\n",
      "Step 154/1000, Meta Loss: 0.4940, Time: 0.56s\n",
      "Step 155/1000, Meta Loss: 0.5370, Time: 0.58s\n",
      "Step 156/1000, Meta Loss: 0.5586, Time: 0.65s\n",
      "Step 157/1000, Meta Loss: 0.5080, Time: 0.70s\n",
      "Step 158/1000, Meta Loss: 0.5116, Time: 0.72s\n",
      "Step 159/1000, Meta Loss: 0.5112, Time: 0.62s\n",
      "Step 160/1000, Meta Loss: 0.5130, Time: 0.59s\n",
      "Step 161/1000, Meta Loss: 0.5286, Time: 0.60s\n",
      "Step 162/1000, Meta Loss: 0.5491, Time: 0.62s\n",
      "Step 163/1000, Meta Loss: 0.5232, Time: 0.61s\n",
      "Step 164/1000, Meta Loss: 0.5108, Time: 0.62s\n",
      "Step 165/1000, Meta Loss: 0.5784, Time: 0.59s\n",
      "Step 166/1000, Meta Loss: 0.5046, Time: 0.60s\n",
      "Step 167/1000, Meta Loss: 0.5164, Time: 0.60s\n",
      "Step 168/1000, Meta Loss: 0.5242, Time: 0.59s\n",
      "Step 169/1000, Meta Loss: 0.5077, Time: 0.58s\n",
      "Step 170/1000, Meta Loss: 0.5229, Time: 0.59s\n",
      "Step 171/1000, Meta Loss: 0.5380, Time: 0.57s\n",
      "Step 172/1000, Meta Loss: 0.5408, Time: 0.58s\n",
      "Step 173/1000, Meta Loss: 0.5392, Time: 0.58s\n",
      "Step 174/1000, Meta Loss: 0.5503, Time: 0.57s\n",
      "Step 175/1000, Meta Loss: 0.5115, Time: 0.59s\n",
      "Step 176/1000, Meta Loss: 0.4976, Time: 0.61s\n",
      "Step 177/1000, Meta Loss: 0.5418, Time: 0.57s\n",
      "Step 178/1000, Meta Loss: 0.5138, Time: 0.56s\n",
      "Step 179/1000, Meta Loss: 0.4755, Time: 0.57s\n",
      "Step 180/1000, Meta Loss: 0.4994, Time: 0.57s\n",
      "Step 181/1000, Meta Loss: 0.4843, Time: 0.58s\n",
      "Step 182/1000, Meta Loss: 0.5386, Time: 0.58s\n",
      "Step 183/1000, Meta Loss: 0.5155, Time: 0.57s\n",
      "Step 184/1000, Meta Loss: 0.5351, Time: 0.69s\n",
      "Step 185/1000, Meta Loss: 0.4988, Time: 0.60s\n",
      "Step 186/1000, Meta Loss: 0.5290, Time: 0.59s\n",
      "Step 187/1000, Meta Loss: 0.5381, Time: 0.58s\n",
      "Step 188/1000, Meta Loss: 0.5102, Time: 0.57s\n",
      "Step 189/1000, Meta Loss: 0.5263, Time: 0.57s\n",
      "Step 190/1000, Meta Loss: 0.4829, Time: 0.57s\n",
      "Step 191/1000, Meta Loss: 0.5216, Time: 0.57s\n",
      "Step 192/1000, Meta Loss: 0.5244, Time: 0.58s\n",
      "Step 193/1000, Meta Loss: 0.4978, Time: 0.58s\n",
      "Step 194/1000, Meta Loss: 0.5118, Time: 0.58s\n",
      "Step 195/1000, Meta Loss: 0.5303, Time: 0.59s\n",
      "Step 196/1000, Meta Loss: 0.5109, Time: 0.58s\n",
      "Step 197/1000, Meta Loss: 0.4664, Time: 0.63s\n",
      "Step 198/1000, Meta Loss: 0.5183, Time: 0.60s\n",
      "Step 199/1000, Meta Loss: 0.5302, Time: 0.66s\n",
      "Step 200/1000, Meta Loss: 0.5065, Time: 0.64s\n",
      "Step 201/1000, Meta Loss: 0.5201, Time: 0.70s\n",
      "Step 202/1000, Meta Loss: 0.5260, Time: 0.59s\n",
      "Step 203/1000, Meta Loss: 0.4949, Time: 0.69s\n",
      "Step 204/1000, Meta Loss: 0.5395, Time: 0.67s\n",
      "Step 205/1000, Meta Loss: 0.4940, Time: 0.60s\n",
      "Step 206/1000, Meta Loss: 0.5175, Time: 0.66s\n",
      "Step 207/1000, Meta Loss: 0.5091, Time: 0.68s\n",
      "Step 208/1000, Meta Loss: 0.4991, Time: 0.61s\n",
      "Step 209/1000, Meta Loss: 0.5330, Time: 0.68s\n",
      "Step 210/1000, Meta Loss: 0.5067, Time: 0.69s\n",
      "Step 211/1000, Meta Loss: 0.5313, Time: 0.78s\n",
      "Step 212/1000, Meta Loss: 0.4846, Time: 0.73s\n",
      "Step 213/1000, Meta Loss: 0.5317, Time: 0.75s\n",
      "Step 214/1000, Meta Loss: 0.5169, Time: 0.65s\n",
      "Step 215/1000, Meta Loss: 0.5380, Time: 0.64s\n",
      "Step 216/1000, Meta Loss: 0.5411, Time: 0.61s\n",
      "Step 217/1000, Meta Loss: 0.5011, Time: 0.72s\n",
      "Step 218/1000, Meta Loss: 0.5269, Time: 0.71s\n",
      "Step 219/1000, Meta Loss: 0.5419, Time: 0.90s\n",
      "Step 220/1000, Meta Loss: 0.5100, Time: 0.64s\n",
      "Step 221/1000, Meta Loss: 0.5083, Time: 0.69s\n",
      "Step 222/1000, Meta Loss: 0.5167, Time: 0.65s\n",
      "Step 223/1000, Meta Loss: 0.4901, Time: 0.66s\n",
      "Step 224/1000, Meta Loss: 0.5120, Time: 0.68s\n",
      "Step 225/1000, Meta Loss: 0.5670, Time: 0.70s\n",
      "Step 226/1000, Meta Loss: 0.5120, Time: 0.58s\n",
      "Step 227/1000, Meta Loss: 0.5054, Time: 0.58s\n",
      "Step 228/1000, Meta Loss: 0.4911, Time: 0.62s\n",
      "Step 229/1000, Meta Loss: 0.5344, Time: 0.61s\n",
      "Step 230/1000, Meta Loss: 0.4936, Time: 0.59s\n",
      "Step 231/1000, Meta Loss: 0.5045, Time: 0.57s\n",
      "Step 232/1000, Meta Loss: 0.5135, Time: 0.56s\n",
      "Step 233/1000, Meta Loss: 0.5302, Time: 0.57s\n",
      "Step 234/1000, Meta Loss: 0.5179, Time: 0.57s\n",
      "Step 235/1000, Meta Loss: 0.5207, Time: 0.55s\n",
      "Step 236/1000, Meta Loss: 0.5294, Time: 0.57s\n",
      "Step 237/1000, Meta Loss: 0.5180, Time: 0.57s\n",
      "Step 238/1000, Meta Loss: 0.5231, Time: 0.57s\n",
      "Step 239/1000, Meta Loss: 0.5281, Time: 0.64s\n",
      "Step 240/1000, Meta Loss: 0.5296, Time: 0.58s\n",
      "Step 241/1000, Meta Loss: 0.5113, Time: 0.56s\n",
      "Step 242/1000, Meta Loss: 0.5264, Time: 0.57s\n",
      "Step 243/1000, Meta Loss: 0.5046, Time: 0.56s\n",
      "Step 244/1000, Meta Loss: 0.5212, Time: 0.56s\n",
      "Step 245/1000, Meta Loss: 0.5276, Time: 0.57s\n",
      "Step 246/1000, Meta Loss: 0.4993, Time: 0.57s\n",
      "Step 247/1000, Meta Loss: 0.5100, Time: 0.57s\n",
      "Step 248/1000, Meta Loss: 0.5942, Time: 0.56s\n",
      "Step 249/1000, Meta Loss: 0.5178, Time: 0.57s\n",
      "Step 250/1000, Meta Loss: 0.4964, Time: 0.57s\n",
      "Step 251/1000, Meta Loss: 0.5329, Time: 0.56s\n",
      "Step 252/1000, Meta Loss: 0.5278, Time: 0.62s\n",
      "Step 253/1000, Meta Loss: 0.5292, Time: 0.58s\n",
      "Step 254/1000, Meta Loss: 0.5035, Time: 0.57s\n",
      "Step 255/1000, Meta Loss: 0.5310, Time: 0.57s\n",
      "Step 256/1000, Meta Loss: 0.5204, Time: 0.57s\n",
      "Step 257/1000, Meta Loss: 0.5325, Time: 0.57s\n",
      "Step 258/1000, Meta Loss: 0.5384, Time: 0.72s\n",
      "Step 259/1000, Meta Loss: 0.4949, Time: 0.59s\n",
      "Step 260/1000, Meta Loss: 0.5645, Time: 0.62s\n",
      "Step 261/1000, Meta Loss: 0.5214, Time: 0.71s\n",
      "Step 262/1000, Meta Loss: 0.5347, Time: 0.77s\n",
      "Step 263/1000, Meta Loss: 0.5176, Time: 0.72s\n",
      "Step 264/1000, Meta Loss: 0.4904, Time: 0.71s\n",
      "Step 265/1000, Meta Loss: 0.5244, Time: 0.61s\n",
      "Step 266/1000, Meta Loss: 0.4993, Time: 0.58s\n",
      "Step 267/1000, Meta Loss: 0.5545, Time: 0.58s\n",
      "Step 268/1000, Meta Loss: 0.5337, Time: 0.58s\n",
      "Step 269/1000, Meta Loss: 0.5652, Time: 0.58s\n",
      "Step 270/1000, Meta Loss: 0.5547, Time: 0.57s\n",
      "Step 271/1000, Meta Loss: 0.5174, Time: 0.67s\n",
      "Step 272/1000, Meta Loss: 0.4958, Time: 0.60s\n",
      "Step 273/1000, Meta Loss: 0.4987, Time: 0.58s\n",
      "Step 274/1000, Meta Loss: 0.4985, Time: 0.63s\n",
      "Step 275/1000, Meta Loss: 0.5055, Time: 0.59s\n",
      "Step 276/1000, Meta Loss: 0.4984, Time: 0.66s\n",
      "Step 277/1000, Meta Loss: 0.5235, Time: 0.60s\n",
      "Step 278/1000, Meta Loss: 0.5380, Time: 0.61s\n",
      "Step 279/1000, Meta Loss: 0.5206, Time: 0.57s\n",
      "Step 280/1000, Meta Loss: 0.5360, Time: 0.57s\n",
      "Step 281/1000, Meta Loss: 0.4847, Time: 0.55s\n",
      "Step 282/1000, Meta Loss: 0.5161, Time: 0.56s\n",
      "Step 283/1000, Meta Loss: 0.5181, Time: 0.64s\n",
      "Step 284/1000, Meta Loss: 0.5419, Time: 0.59s\n",
      "Step 285/1000, Meta Loss: 0.4974, Time: 0.71s\n",
      "Step 286/1000, Meta Loss: 0.5175, Time: 0.60s\n",
      "Step 287/1000, Meta Loss: 0.4664, Time: 0.60s\n",
      "Step 288/1000, Meta Loss: 0.5171, Time: 0.61s\n",
      "Step 289/1000, Meta Loss: 0.5287, Time: 0.61s\n",
      "Step 290/1000, Meta Loss: 0.5351, Time: 0.58s\n",
      "Step 291/1000, Meta Loss: 0.4907, Time: 0.60s\n",
      "Step 292/1000, Meta Loss: 0.5334, Time: 0.58s\n",
      "Step 293/1000, Meta Loss: 0.5342, Time: 0.60s\n",
      "Step 294/1000, Meta Loss: 0.5283, Time: 0.60s\n",
      "Step 295/1000, Meta Loss: 0.5397, Time: 0.59s\n",
      "Step 296/1000, Meta Loss: 0.5130, Time: 0.58s\n",
      "Step 297/1000, Meta Loss: 0.4882, Time: 0.56s\n",
      "Step 298/1000, Meta Loss: 0.5000, Time: 0.57s\n",
      "Step 299/1000, Meta Loss: 0.5211, Time: 0.58s\n",
      "Step 300/1000, Meta Loss: 0.5069, Time: 0.56s\n",
      "Step 301/1000, Meta Loss: 0.5079, Time: 0.61s\n",
      "Step 302/1000, Meta Loss: 0.4983, Time: 0.58s\n",
      "Step 303/1000, Meta Loss: 0.5265, Time: 0.61s\n",
      "Step 304/1000, Meta Loss: 0.5016, Time: 0.56s\n",
      "Step 305/1000, Meta Loss: 0.5306, Time: 0.58s\n",
      "Step 306/1000, Meta Loss: 0.5071, Time: 0.60s\n",
      "Step 307/1000, Meta Loss: 0.5290, Time: 0.65s\n",
      "Step 308/1000, Meta Loss: 0.4884, Time: 0.69s\n",
      "Step 309/1000, Meta Loss: 0.5320, Time: 0.61s\n",
      "Step 310/1000, Meta Loss: 0.5412, Time: 0.59s\n",
      "Step 311/1000, Meta Loss: 0.5030, Time: 0.59s\n",
      "Step 312/1000, Meta Loss: 0.5569, Time: 0.58s\n",
      "Step 313/1000, Meta Loss: 0.5281, Time: 0.60s\n",
      "Step 314/1000, Meta Loss: 0.5108, Time: 0.60s\n",
      "Step 315/1000, Meta Loss: 0.5138, Time: 0.59s\n",
      "Step 316/1000, Meta Loss: 0.5119, Time: 0.58s\n",
      "Step 317/1000, Meta Loss: 0.5375, Time: 0.58s\n",
      "Step 318/1000, Meta Loss: 0.5236, Time: 0.57s\n",
      "Step 319/1000, Meta Loss: 0.5386, Time: 0.57s\n",
      "Step 320/1000, Meta Loss: 0.5095, Time: 0.57s\n",
      "Step 321/1000, Meta Loss: 0.5505, Time: 0.57s\n",
      "Step 322/1000, Meta Loss: 0.5334, Time: 0.57s\n",
      "Step 323/1000, Meta Loss: 0.5007, Time: 0.56s\n",
      "Step 324/1000, Meta Loss: 0.5429, Time: 0.58s\n",
      "Step 325/1000, Meta Loss: 0.5679, Time: 0.57s\n",
      "Step 326/1000, Meta Loss: 0.5332, Time: 0.56s\n",
      "Step 327/1000, Meta Loss: 0.5194, Time: 0.56s\n",
      "Step 328/1000, Meta Loss: 0.5149, Time: 0.56s\n",
      "Step 329/1000, Meta Loss: 0.5249, Time: 0.56s\n",
      "Step 330/1000, Meta Loss: 0.5445, Time: 0.60s\n",
      "Step 331/1000, Meta Loss: 0.5096, Time: 0.64s\n",
      "Step 332/1000, Meta Loss: 0.5116, Time: 0.58s\n",
      "Step 333/1000, Meta Loss: 0.5410, Time: 0.66s\n",
      "Step 334/1000, Meta Loss: 0.4854, Time: 0.61s\n",
      "Step 335/1000, Meta Loss: 0.5366, Time: 0.62s\n",
      "Step 336/1000, Meta Loss: 0.4927, Time: 0.60s\n",
      "Step 337/1000, Meta Loss: 0.5556, Time: 0.58s\n",
      "Step 338/1000, Meta Loss: 0.5328, Time: 0.59s\n",
      "Step 339/1000, Meta Loss: 0.5018, Time: 0.59s\n",
      "Step 340/1000, Meta Loss: 0.5232, Time: 0.58s\n",
      "Step 341/1000, Meta Loss: 0.4915, Time: 0.59s\n",
      "Step 342/1000, Meta Loss: 0.5478, Time: 0.60s\n",
      "Step 343/1000, Meta Loss: 0.5424, Time: 0.61s\n",
      "Step 344/1000, Meta Loss: 0.5141, Time: 0.62s\n",
      "Step 345/1000, Meta Loss: 0.5297, Time: 0.61s\n",
      "Step 346/1000, Meta Loss: 0.5151, Time: 0.60s\n",
      "Step 347/1000, Meta Loss: 0.5686, Time: 0.58s\n",
      "Step 348/1000, Meta Loss: 0.5258, Time: 0.57s\n",
      "Step 349/1000, Meta Loss: 0.5311, Time: 0.60s\n",
      "Step 350/1000, Meta Loss: 0.5045, Time: 0.57s\n",
      "Step 351/1000, Meta Loss: 0.5184, Time: 0.58s\n",
      "Step 352/1000, Meta Loss: 0.5224, Time: 0.54s\n",
      "Step 353/1000, Meta Loss: 0.5251, Time: 0.55s\n",
      "Step 354/1000, Meta Loss: 0.5238, Time: 0.56s\n",
      "Step 355/1000, Meta Loss: 0.5020, Time: 0.63s\n",
      "Step 356/1000, Meta Loss: 0.5491, Time: 0.71s\n",
      "Step 357/1000, Meta Loss: 0.4935, Time: 0.65s\n",
      "Step 358/1000, Meta Loss: 0.5191, Time: 0.61s\n",
      "Step 359/1000, Meta Loss: 0.5518, Time: 0.59s\n",
      "Step 360/1000, Meta Loss: 0.5602, Time: 0.59s\n",
      "Step 361/1000, Meta Loss: 0.5189, Time: 0.59s\n",
      "Step 362/1000, Meta Loss: 0.5067, Time: 0.67s\n",
      "Step 363/1000, Meta Loss: 0.5330, Time: 0.59s\n",
      "Step 364/1000, Meta Loss: 0.5402, Time: 0.63s\n",
      "Step 365/1000, Meta Loss: 0.4959, Time: 0.62s\n",
      "Step 366/1000, Meta Loss: 0.5013, Time: 0.66s\n",
      "Step 367/1000, Meta Loss: 0.4810, Time: 0.68s\n",
      "Step 368/1000, Meta Loss: 0.5040, Time: 0.59s\n",
      "Step 369/1000, Meta Loss: 0.4973, Time: 0.68s\n",
      "Step 370/1000, Meta Loss: 0.5170, Time: 0.63s\n",
      "Step 371/1000, Meta Loss: 0.5210, Time: 0.61s\n",
      "Step 372/1000, Meta Loss: 0.4862, Time: 0.61s\n",
      "Step 373/1000, Meta Loss: 0.5284, Time: 0.64s\n",
      "Step 374/1000, Meta Loss: 0.4900, Time: 0.73s\n",
      "Step 375/1000, Meta Loss: 0.5095, Time: 0.73s\n",
      "Step 376/1000, Meta Loss: 0.4965, Time: 0.62s\n",
      "Step 377/1000, Meta Loss: 0.5505, Time: 0.57s\n",
      "Step 378/1000, Meta Loss: 0.5406, Time: 0.58s\n",
      "Step 379/1000, Meta Loss: 0.4762, Time: 0.61s\n",
      "Step 380/1000, Meta Loss: 0.5230, Time: 0.78s\n",
      "Step 381/1000, Meta Loss: 0.5189, Time: 0.68s\n",
      "Step 382/1000, Meta Loss: 0.5091, Time: 0.63s\n",
      "Step 383/1000, Meta Loss: 0.5207, Time: 0.66s\n",
      "Step 384/1000, Meta Loss: 0.5131, Time: 0.76s\n",
      "Step 385/1000, Meta Loss: 0.5014, Time: 0.77s\n",
      "Step 386/1000, Meta Loss: 0.5430, Time: 0.60s\n",
      "Step 387/1000, Meta Loss: 0.5449, Time: 0.56s\n",
      "Step 388/1000, Meta Loss: 0.5455, Time: 0.58s\n",
      "Step 389/1000, Meta Loss: 0.4867, Time: 0.59s\n",
      "Step 390/1000, Meta Loss: 0.5110, Time: 0.58s\n",
      "Step 391/1000, Meta Loss: 0.5148, Time: 0.57s\n",
      "Step 392/1000, Meta Loss: 0.5063, Time: 0.57s\n",
      "Step 393/1000, Meta Loss: 0.5559, Time: 0.56s\n",
      "Step 394/1000, Meta Loss: 0.5034, Time: 0.58s\n",
      "Step 395/1000, Meta Loss: 0.5343, Time: 0.75s\n",
      "Step 396/1000, Meta Loss: 0.5216, Time: 0.74s\n",
      "Step 397/1000, Meta Loss: 0.4897, Time: 0.60s\n",
      "Step 398/1000, Meta Loss: 0.5306, Time: 0.58s\n",
      "Step 399/1000, Meta Loss: 0.5776, Time: 0.57s\n",
      "Step 400/1000, Meta Loss: 0.5726, Time: 0.57s\n",
      "Step 401/1000, Meta Loss: 0.5149, Time: 0.58s\n",
      "Step 402/1000, Meta Loss: 0.5310, Time: 0.58s\n",
      "Step 403/1000, Meta Loss: 0.5304, Time: 0.59s\n",
      "Step 404/1000, Meta Loss: 0.5005, Time: 0.64s\n",
      "Step 405/1000, Meta Loss: 0.5129, Time: 0.60s\n",
      "Step 406/1000, Meta Loss: 0.5349, Time: 0.58s\n",
      "Step 407/1000, Meta Loss: 0.5150, Time: 0.59s\n",
      "Step 408/1000, Meta Loss: 0.5214, Time: 0.57s\n",
      "Step 409/1000, Meta Loss: 0.5267, Time: 0.57s\n",
      "Step 410/1000, Meta Loss: 0.5280, Time: 0.57s\n",
      "Step 411/1000, Meta Loss: 0.5074, Time: 0.57s\n",
      "Step 412/1000, Meta Loss: 0.5303, Time: 0.57s\n",
      "Step 413/1000, Meta Loss: 0.5349, Time: 0.57s\n",
      "Step 414/1000, Meta Loss: 0.5464, Time: 0.58s\n",
      "Step 415/1000, Meta Loss: 0.5404, Time: 0.57s\n",
      "Step 416/1000, Meta Loss: 0.5459, Time: 0.65s\n",
      "Step 417/1000, Meta Loss: 0.5237, Time: 0.59s\n",
      "Step 418/1000, Meta Loss: 0.5372, Time: 0.58s\n",
      "Step 419/1000, Meta Loss: 0.5361, Time: 0.58s\n",
      "Step 420/1000, Meta Loss: 0.5174, Time: 0.57s\n",
      "Step 421/1000, Meta Loss: 0.5388, Time: 0.57s\n",
      "Step 422/1000, Meta Loss: 0.5881, Time: 0.57s\n",
      "Step 423/1000, Meta Loss: 0.4874, Time: 0.56s\n",
      "Step 424/1000, Meta Loss: 0.5169, Time: 0.58s\n",
      "Step 425/1000, Meta Loss: 0.5306, Time: 0.62s\n",
      "Step 426/1000, Meta Loss: 0.5339, Time: 0.61s\n",
      "Step 427/1000, Meta Loss: 0.5105, Time: 0.63s\n",
      "Step 428/1000, Meta Loss: 0.5129, Time: 0.62s\n",
      "Step 429/1000, Meta Loss: 0.5213, Time: 0.62s\n",
      "Step 430/1000, Meta Loss: 0.5505, Time: 0.59s\n",
      "Step 431/1000, Meta Loss: 0.5370, Time: 0.58s\n",
      "Step 432/1000, Meta Loss: 0.5254, Time: 0.59s\n",
      "Step 433/1000, Meta Loss: 0.5105, Time: 0.71s\n",
      "Step 434/1000, Meta Loss: 0.4981, Time: 0.66s\n",
      "Step 435/1000, Meta Loss: 0.5455, Time: 0.76s\n",
      "Step 436/1000, Meta Loss: 0.5449, Time: 0.95s\n",
      "Step 437/1000, Meta Loss: 0.4927, Time: 0.79s\n",
      "Step 438/1000, Meta Loss: 0.5517, Time: 0.57s\n",
      "Step 439/1000, Meta Loss: 0.5230, Time: 0.74s\n",
      "Step 440/1000, Meta Loss: 0.5366, Time: 0.71s\n",
      "Step 441/1000, Meta Loss: 0.5326, Time: 0.72s\n",
      "Step 442/1000, Meta Loss: 0.5291, Time: 0.62s\n",
      "Step 443/1000, Meta Loss: 0.5185, Time: 0.64s\n",
      "Step 444/1000, Meta Loss: 0.5367, Time: 0.66s\n",
      "Step 445/1000, Meta Loss: 0.5245, Time: 0.60s\n",
      "Step 446/1000, Meta Loss: 0.5050, Time: 0.59s\n",
      "Step 447/1000, Meta Loss: 0.5052, Time: 0.60s\n",
      "Step 448/1000, Meta Loss: 0.4810, Time: 0.60s\n",
      "Step 449/1000, Meta Loss: 0.5012, Time: 0.66s\n",
      "Step 450/1000, Meta Loss: 0.5161, Time: 0.76s\n",
      "Step 451/1000, Meta Loss: 0.5005, Time: 0.63s\n",
      "Step 452/1000, Meta Loss: 0.5196, Time: 0.69s\n",
      "Step 453/1000, Meta Loss: 0.5610, Time: 0.69s\n",
      "Step 454/1000, Meta Loss: 0.5014, Time: 0.57s\n",
      "Step 455/1000, Meta Loss: 0.5367, Time: 0.62s\n",
      "Step 456/1000, Meta Loss: 0.5603, Time: 0.73s\n",
      "Step 457/1000, Meta Loss: 0.5506, Time: 0.60s\n",
      "Step 458/1000, Meta Loss: 0.5454, Time: 0.68s\n",
      "Step 459/1000, Meta Loss: 0.4826, Time: 0.68s\n",
      "Step 460/1000, Meta Loss: 0.5108, Time: 0.66s\n",
      "Step 461/1000, Meta Loss: 0.5088, Time: 0.65s\n",
      "Step 462/1000, Meta Loss: 0.5445, Time: 0.73s\n",
      "Step 463/1000, Meta Loss: 0.5349, Time: 0.68s\n",
      "Step 464/1000, Meta Loss: 0.5135, Time: 0.60s\n",
      "Step 465/1000, Meta Loss: 0.5437, Time: 0.64s\n",
      "Step 466/1000, Meta Loss: 0.5294, Time: 0.76s\n",
      "Step 467/1000, Meta Loss: 0.5322, Time: 0.63s\n",
      "Step 468/1000, Meta Loss: 0.5463, Time: 0.83s\n",
      "Step 469/1000, Meta Loss: 0.5404, Time: 0.83s\n",
      "Step 470/1000, Meta Loss: 0.5201, Time: 0.68s\n",
      "Step 471/1000, Meta Loss: 0.5490, Time: 0.61s\n",
      "Step 472/1000, Meta Loss: 0.4892, Time: 0.58s\n",
      "Step 473/1000, Meta Loss: 0.5520, Time: 0.57s\n",
      "Step 474/1000, Meta Loss: 0.5237, Time: 0.57s\n",
      "Step 475/1000, Meta Loss: 0.5728, Time: 0.56s\n",
      "Step 476/1000, Meta Loss: 0.5466, Time: 0.61s\n",
      "Step 477/1000, Meta Loss: 0.5208, Time: 0.57s\n",
      "Step 478/1000, Meta Loss: 0.5567, Time: 0.70s\n",
      "Step 479/1000, Meta Loss: 0.5078, Time: 0.64s\n",
      "Step 480/1000, Meta Loss: 0.5241, Time: 0.73s\n",
      "Step 481/1000, Meta Loss: 0.5524, Time: 0.59s\n",
      "Step 482/1000, Meta Loss: 0.5145, Time: 0.63s\n",
      "Step 483/1000, Meta Loss: 0.4918, Time: 0.56s\n",
      "Step 484/1000, Meta Loss: 0.5155, Time: 0.57s\n",
      "Step 485/1000, Meta Loss: 0.5310, Time: 0.61s\n",
      "Step 486/1000, Meta Loss: 0.5111, Time: 0.60s\n",
      "Step 487/1000, Meta Loss: 0.4954, Time: 0.60s\n",
      "Step 488/1000, Meta Loss: 0.5196, Time: 0.60s\n",
      "Step 489/1000, Meta Loss: 0.4977, Time: 0.56s\n",
      "Step 490/1000, Meta Loss: 0.5131, Time: 0.60s\n",
      "Step 491/1000, Meta Loss: 0.5020, Time: 0.60s\n",
      "Step 492/1000, Meta Loss: 0.5181, Time: 0.61s\n",
      "Step 493/1000, Meta Loss: 0.5006, Time: 0.55s\n",
      "Step 494/1000, Meta Loss: 0.5238, Time: 0.56s\n",
      "Step 495/1000, Meta Loss: 0.5158, Time: 0.54s\n",
      "Step 496/1000, Meta Loss: 0.4674, Time: 0.57s\n",
      "Step 497/1000, Meta Loss: 0.5334, Time: 0.78s\n",
      "Step 498/1000, Meta Loss: 0.5261, Time: 0.59s\n",
      "Step 499/1000, Meta Loss: 0.5342, Time: 0.57s\n",
      "Step 500/1000, Meta Loss: 0.5022, Time: 0.56s\n",
      "Step 501/1000, Meta Loss: 0.5113, Time: 0.60s\n",
      "Step 502/1000, Meta Loss: 0.5394, Time: 0.62s\n",
      "Step 503/1000, Meta Loss: 0.5366, Time: 0.75s\n",
      "Step 504/1000, Meta Loss: 0.5167, Time: 0.91s\n",
      "Step 505/1000, Meta Loss: 0.5099, Time: 0.60s\n",
      "Step 506/1000, Meta Loss: 0.5424, Time: 0.76s\n",
      "Step 507/1000, Meta Loss: 0.5512, Time: 0.62s\n",
      "Step 508/1000, Meta Loss: 0.5125, Time: 0.60s\n",
      "Step 509/1000, Meta Loss: 0.5615, Time: 0.60s\n",
      "Step 510/1000, Meta Loss: 0.5312, Time: 0.87s\n",
      "Step 511/1000, Meta Loss: 0.5141, Time: 0.79s\n",
      "Step 512/1000, Meta Loss: 0.6131, Time: 0.80s\n",
      "Step 513/1000, Meta Loss: 0.4981, Time: 0.64s\n",
      "Step 514/1000, Meta Loss: 0.4770, Time: 0.91s\n",
      "Step 515/1000, Meta Loss: 0.5213, Time: 0.76s\n",
      "Step 516/1000, Meta Loss: 0.5016, Time: 0.57s\n",
      "Step 517/1000, Meta Loss: 0.5565, Time: 0.76s\n",
      "Step 518/1000, Meta Loss: 0.5117, Time: 0.61s\n",
      "Step 519/1000, Meta Loss: 0.5262, Time: 0.55s\n",
      "Step 520/1000, Meta Loss: 0.5125, Time: 0.68s\n",
      "Step 521/1000, Meta Loss: 0.5231, Time: 0.64s\n",
      "Step 522/1000, Meta Loss: 0.4952, Time: 0.57s\n",
      "Step 523/1000, Meta Loss: 0.5199, Time: 0.57s\n",
      "Step 524/1000, Meta Loss: 0.5575, Time: 0.55s\n",
      "Step 525/1000, Meta Loss: 0.5084, Time: 0.58s\n",
      "Step 526/1000, Meta Loss: 0.5411, Time: 0.55s\n",
      "Step 527/1000, Meta Loss: 0.5075, Time: 0.69s\n",
      "Step 528/1000, Meta Loss: 0.5128, Time: 0.59s\n",
      "Step 529/1000, Meta Loss: 0.5221, Time: 0.56s\n",
      "Step 530/1000, Meta Loss: 0.4793, Time: 0.62s\n",
      "Step 531/1000, Meta Loss: 0.5135, Time: 0.77s\n",
      "Step 532/1000, Meta Loss: 0.5092, Time: 0.71s\n",
      "Step 533/1000, Meta Loss: 0.5419, Time: 0.65s\n",
      "Step 534/1000, Meta Loss: 0.5353, Time: 0.68s\n",
      "Step 535/1000, Meta Loss: 0.5535, Time: 0.59s\n",
      "Step 536/1000, Meta Loss: 0.4955, Time: 0.57s\n",
      "Step 537/1000, Meta Loss: 0.5590, Time: 0.60s\n",
      "Step 538/1000, Meta Loss: 0.4886, Time: 0.59s\n",
      "Step 539/1000, Meta Loss: 0.5091, Time: 0.57s\n",
      "Step 540/1000, Meta Loss: 0.5473, Time: 0.56s\n",
      "Step 541/1000, Meta Loss: 0.5367, Time: 0.56s\n",
      "Step 542/1000, Meta Loss: 0.5180, Time: 0.57s\n",
      "Step 543/1000, Meta Loss: 0.5348, Time: 0.56s\n",
      "Step 544/1000, Meta Loss: 0.5167, Time: 0.88s\n",
      "Step 545/1000, Meta Loss: 0.4759, Time: 0.60s\n",
      "Step 546/1000, Meta Loss: 0.5371, Time: 0.72s\n",
      "Step 547/1000, Meta Loss: 0.4729, Time: 0.63s\n",
      "Step 548/1000, Meta Loss: 0.5128, Time: 0.69s\n",
      "Step 549/1000, Meta Loss: 0.5337, Time: 0.62s\n",
      "Step 550/1000, Meta Loss: 0.5029, Time: 0.58s\n",
      "Step 551/1000, Meta Loss: 0.5102, Time: 0.64s\n",
      "Step 552/1000, Meta Loss: 0.4982, Time: 0.80s\n",
      "Step 553/1000, Meta Loss: 0.5228, Time: 0.58s\n",
      "Step 554/1000, Meta Loss: 0.5091, Time: 0.58s\n",
      "Step 555/1000, Meta Loss: 0.4968, Time: 0.57s\n",
      "Step 556/1000, Meta Loss: 0.5019, Time: 0.58s\n",
      "Step 557/1000, Meta Loss: 0.5069, Time: 0.66s\n",
      "Step 558/1000, Meta Loss: 0.4988, Time: 0.59s\n",
      "Step 559/1000, Meta Loss: 0.5187, Time: 0.80s\n",
      "Step 560/1000, Meta Loss: 0.5285, Time: 0.71s\n",
      "Step 561/1000, Meta Loss: 0.5299, Time: 0.62s\n",
      "Step 562/1000, Meta Loss: 0.5425, Time: 0.60s\n",
      "Step 563/1000, Meta Loss: 0.5169, Time: 0.59s\n",
      "Step 564/1000, Meta Loss: 0.4905, Time: 0.68s\n",
      "Step 565/1000, Meta Loss: 0.5006, Time: 0.76s\n",
      "Step 566/1000, Meta Loss: 0.5111, Time: 0.64s\n",
      "Step 567/1000, Meta Loss: 0.5120, Time: 0.61s\n",
      "Step 568/1000, Meta Loss: 0.5092, Time: 0.57s\n",
      "Step 569/1000, Meta Loss: 0.5091, Time: 0.65s\n",
      "Step 570/1000, Meta Loss: 0.5323, Time: 0.60s\n",
      "Step 571/1000, Meta Loss: 0.5183, Time: 0.58s\n",
      "Step 572/1000, Meta Loss: 0.5654, Time: 0.56s\n",
      "Step 573/1000, Meta Loss: 0.5303, Time: 0.56s\n",
      "Step 574/1000, Meta Loss: 0.5114, Time: 0.57s\n",
      "Step 575/1000, Meta Loss: 0.5287, Time: 0.57s\n",
      "Step 576/1000, Meta Loss: 0.5576, Time: 0.56s\n",
      "Step 577/1000, Meta Loss: 0.5359, Time: 0.56s\n",
      "Step 578/1000, Meta Loss: 0.5233, Time: 0.55s\n",
      "Step 579/1000, Meta Loss: 0.5535, Time: 0.55s\n",
      "Step 580/1000, Meta Loss: 0.5041, Time: 0.56s\n",
      "Step 581/1000, Meta Loss: 0.5592, Time: 0.58s\n",
      "Step 582/1000, Meta Loss: 0.5181, Time: 0.56s\n",
      "Step 583/1000, Meta Loss: 0.5276, Time: 0.58s\n",
      "Step 584/1000, Meta Loss: 0.5086, Time: 0.56s\n",
      "Step 585/1000, Meta Loss: 0.5097, Time: 0.73s\n",
      "Step 586/1000, Meta Loss: 0.5160, Time: 0.77s\n",
      "Step 587/1000, Meta Loss: 0.5015, Time: 0.82s\n",
      "Step 588/1000, Meta Loss: 0.5004, Time: 0.67s\n",
      "Step 589/1000, Meta Loss: 0.5135, Time: 0.60s\n",
      "Step 590/1000, Meta Loss: 0.5243, Time: 0.58s\n",
      "Step 591/1000, Meta Loss: 0.5340, Time: 0.58s\n",
      "Step 592/1000, Meta Loss: 0.5759, Time: 0.57s\n",
      "Step 593/1000, Meta Loss: 0.5312, Time: 0.58s\n",
      "Step 594/1000, Meta Loss: 0.5019, Time: 0.66s\n",
      "Step 595/1000, Meta Loss: 0.5467, Time: 0.58s\n",
      "Step 596/1000, Meta Loss: 0.4915, Time: 0.71s\n",
      "Step 597/1000, Meta Loss: 0.5702, Time: 0.59s\n",
      "Step 598/1000, Meta Loss: 0.5355, Time: 0.60s\n",
      "Step 599/1000, Meta Loss: 0.5197, Time: 0.61s\n",
      "Step 600/1000, Meta Loss: 0.4929, Time: 0.59s\n",
      "Step 601/1000, Meta Loss: 0.5197, Time: 0.58s\n",
      "Step 602/1000, Meta Loss: 0.4979, Time: 0.57s\n",
      "Step 603/1000, Meta Loss: 0.5273, Time: 0.57s\n",
      "Step 604/1000, Meta Loss: 0.5346, Time: 0.57s\n",
      "Step 605/1000, Meta Loss: 0.5506, Time: 0.59s\n",
      "Step 606/1000, Meta Loss: 0.5134, Time: 0.58s\n",
      "Step 607/1000, Meta Loss: 0.5390, Time: 0.58s\n",
      "Step 608/1000, Meta Loss: 0.5038, Time: 0.67s\n",
      "Step 609/1000, Meta Loss: 0.5585, Time: 0.80s\n",
      "Step 610/1000, Meta Loss: 0.5523, Time: 0.61s\n",
      "Step 611/1000, Meta Loss: 0.4866, Time: 0.58s\n",
      "Step 612/1000, Meta Loss: 0.5257, Time: 0.58s\n",
      "Step 613/1000, Meta Loss: 0.5387, Time: 0.75s\n",
      "Step 614/1000, Meta Loss: 0.5288, Time: 0.76s\n",
      "Step 615/1000, Meta Loss: 0.5161, Time: 0.80s\n",
      "Step 616/1000, Meta Loss: 0.5381, Time: 0.69s\n",
      "Step 617/1000, Meta Loss: 0.5292, Time: 0.71s\n",
      "Step 618/1000, Meta Loss: 0.5073, Time: 0.71s\n",
      "Step 619/1000, Meta Loss: 0.5295, Time: 0.73s\n",
      "Step 620/1000, Meta Loss: 0.5178, Time: 0.72s\n",
      "Step 621/1000, Meta Loss: 0.5278, Time: 0.79s\n",
      "Step 622/1000, Meta Loss: 0.5672, Time: 0.58s\n",
      "Step 623/1000, Meta Loss: 0.5209, Time: 0.76s\n",
      "Step 624/1000, Meta Loss: 0.5065, Time: 0.71s\n",
      "Step 625/1000, Meta Loss: 0.5461, Time: 0.78s\n",
      "Step 626/1000, Meta Loss: 0.5141, Time: 0.80s\n",
      "Step 627/1000, Meta Loss: 0.4608, Time: 0.80s\n",
      "Step 628/1000, Meta Loss: 0.5089, Time: 0.82s\n",
      "Step 629/1000, Meta Loss: 0.4825, Time: 0.57s\n",
      "Step 630/1000, Meta Loss: 0.5109, Time: 0.71s\n",
      "Step 631/1000, Meta Loss: 0.4904, Time: 0.76s\n",
      "Step 632/1000, Meta Loss: 0.5134, Time: 0.78s\n",
      "Step 633/1000, Meta Loss: 0.5272, Time: 0.76s\n",
      "Step 634/1000, Meta Loss: 0.4871, Time: 0.87s\n",
      "Step 635/1000, Meta Loss: 0.5485, Time: 0.91s\n",
      "Step 636/1000, Meta Loss: 0.5043, Time: 0.59s\n",
      "Step 637/1000, Meta Loss: 0.5274, Time: 0.66s\n",
      "Step 638/1000, Meta Loss: 0.4843, Time: 0.59s\n",
      "Step 639/1000, Meta Loss: 0.5120, Time: 0.58s\n",
      "Step 640/1000, Meta Loss: 0.5253, Time: 0.57s\n",
      "Step 641/1000, Meta Loss: 0.5223, Time: 0.56s\n",
      "Step 642/1000, Meta Loss: 0.5597, Time: 0.56s\n",
      "Step 643/1000, Meta Loss: 0.5294, Time: 0.65s\n",
      "Step 644/1000, Meta Loss: 0.5295, Time: 0.78s\n",
      "Step 645/1000, Meta Loss: 0.4930, Time: 0.93s\n",
      "Step 646/1000, Meta Loss: 0.5114, Time: 0.62s\n",
      "Step 647/1000, Meta Loss: 0.5166, Time: 0.73s\n",
      "Step 648/1000, Meta Loss: 0.5129, Time: 0.74s\n",
      "Step 649/1000, Meta Loss: 0.5181, Time: 0.83s\n",
      "Step 650/1000, Meta Loss: 0.5441, Time: 0.74s\n",
      "Step 651/1000, Meta Loss: 0.4903, Time: 0.65s\n",
      "Step 652/1000, Meta Loss: 0.4951, Time: 0.74s\n",
      "Step 653/1000, Meta Loss: 0.5330, Time: 0.72s\n",
      "Step 654/1000, Meta Loss: 0.5240, Time: 0.80s\n",
      "Step 655/1000, Meta Loss: 0.5112, Time: 0.67s\n",
      "Step 656/1000, Meta Loss: 0.5281, Time: 0.73s\n",
      "Step 657/1000, Meta Loss: 0.5242, Time: 0.71s\n",
      "Step 658/1000, Meta Loss: 0.4964, Time: 0.74s\n",
      "Step 659/1000, Meta Loss: 0.5510, Time: 0.61s\n",
      "Step 660/1000, Meta Loss: 0.5169, Time: 0.75s\n",
      "Step 661/1000, Meta Loss: 0.5616, Time: 0.72s\n",
      "Step 662/1000, Meta Loss: 0.5085, Time: 0.60s\n",
      "Step 663/1000, Meta Loss: 0.5727, Time: 0.58s\n",
      "Step 664/1000, Meta Loss: 0.5328, Time: 0.66s\n",
      "Step 665/1000, Meta Loss: 0.5002, Time: 0.82s\n",
      "Step 666/1000, Meta Loss: 0.5140, Time: 0.74s\n",
      "Step 667/1000, Meta Loss: 0.5250, Time: 0.73s\n",
      "Step 668/1000, Meta Loss: 0.4839, Time: 0.73s\n",
      "Step 669/1000, Meta Loss: 0.5385, Time: 0.78s\n",
      "Step 670/1000, Meta Loss: 0.5305, Time: 0.75s\n",
      "Step 671/1000, Meta Loss: 0.5319, Time: 0.65s\n",
      "Step 672/1000, Meta Loss: 0.5365, Time: 0.65s\n",
      "Step 673/1000, Meta Loss: 0.5692, Time: 0.60s\n",
      "Step 674/1000, Meta Loss: 0.5001, Time: 0.60s\n",
      "Step 675/1000, Meta Loss: 0.5573, Time: 0.60s\n",
      "Step 676/1000, Meta Loss: 0.5095, Time: 0.73s\n",
      "Step 677/1000, Meta Loss: 0.5423, Time: 0.67s\n",
      "Step 678/1000, Meta Loss: 0.5294, Time: 0.72s\n",
      "Step 679/1000, Meta Loss: 0.4874, Time: 0.67s\n",
      "Step 680/1000, Meta Loss: 0.5356, Time: 0.60s\n",
      "Step 681/1000, Meta Loss: 0.5184, Time: 0.61s\n",
      "Step 682/1000, Meta Loss: 0.5033, Time: 0.59s\n",
      "Step 683/1000, Meta Loss: 0.5086, Time: 0.57s\n",
      "Step 684/1000, Meta Loss: 0.5094, Time: 0.77s\n",
      "Step 685/1000, Meta Loss: 0.4608, Time: 0.61s\n",
      "Step 686/1000, Meta Loss: 0.5267, Time: 0.58s\n",
      "Step 687/1000, Meta Loss: 0.5547, Time: 0.57s\n",
      "Step 688/1000, Meta Loss: 0.5096, Time: 0.56s\n",
      "Step 689/1000, Meta Loss: 0.4918, Time: 0.60s\n",
      "Step 690/1000, Meta Loss: 0.5302, Time: 0.57s\n",
      "Step 691/1000, Meta Loss: 0.5463, Time: 0.56s\n",
      "Step 692/1000, Meta Loss: 0.5473, Time: 0.57s\n",
      "Step 693/1000, Meta Loss: 0.5165, Time: 0.58s\n",
      "Step 694/1000, Meta Loss: 0.4766, Time: 0.56s\n",
      "Step 695/1000, Meta Loss: 0.5285, Time: 0.56s\n",
      "Step 696/1000, Meta Loss: 0.5248, Time: 0.57s\n",
      "Step 697/1000, Meta Loss: 0.5170, Time: 0.57s\n",
      "Step 698/1000, Meta Loss: 0.5746, Time: 0.57s\n",
      "Step 699/1000, Meta Loss: 0.5457, Time: 0.56s\n",
      "Step 700/1000, Meta Loss: 0.5442, Time: 0.57s\n",
      "Step 701/1000, Meta Loss: 0.5463, Time: 0.65s\n",
      "Step 702/1000, Meta Loss: 0.5145, Time: 0.62s\n",
      "Step 703/1000, Meta Loss: 0.4971, Time: 0.60s\n",
      "Step 704/1000, Meta Loss: 0.5158, Time: 0.58s\n",
      "Step 705/1000, Meta Loss: 0.5163, Time: 0.61s\n",
      "Step 706/1000, Meta Loss: 0.5429, Time: 0.59s\n",
      "Step 707/1000, Meta Loss: 0.5121, Time: 0.58s\n",
      "Step 708/1000, Meta Loss: 0.5143, Time: 0.78s\n",
      "Step 709/1000, Meta Loss: 0.5225, Time: 0.69s\n",
      "Step 710/1000, Meta Loss: 0.5403, Time: 0.64s\n",
      "Step 711/1000, Meta Loss: 0.5389, Time: 0.74s\n",
      "Step 712/1000, Meta Loss: 0.5260, Time: 0.74s\n",
      "Step 713/1000, Meta Loss: 0.5568, Time: 0.65s\n",
      "Step 714/1000, Meta Loss: 0.5661, Time: 0.80s\n",
      "Step 715/1000, Meta Loss: 0.5075, Time: 0.58s\n",
      "Step 716/1000, Meta Loss: 0.5427, Time: 0.60s\n",
      "Step 717/1000, Meta Loss: 0.5273, Time: 0.60s\n",
      "Step 718/1000, Meta Loss: 0.5113, Time: 0.67s\n",
      "Step 719/1000, Meta Loss: 0.5097, Time: 0.61s\n",
      "Step 720/1000, Meta Loss: 0.5213, Time: 0.77s\n",
      "Step 721/1000, Meta Loss: 0.4973, Time: 0.67s\n",
      "Step 722/1000, Meta Loss: 0.5181, Time: 0.82s\n",
      "Step 723/1000, Meta Loss: 0.4951, Time: 0.66s\n",
      "Step 724/1000, Meta Loss: 0.5430, Time: 0.78s\n",
      "Step 725/1000, Meta Loss: 0.5123, Time: 0.69s\n",
      "Step 726/1000, Meta Loss: 0.5558, Time: 0.76s\n",
      "Step 727/1000, Meta Loss: 0.5192, Time: 0.59s\n",
      "Step 728/1000, Meta Loss: 0.5358, Time: 0.63s\n",
      "Step 729/1000, Meta Loss: 0.4774, Time: 0.62s\n",
      "Step 730/1000, Meta Loss: 0.4972, Time: 0.74s\n",
      "Step 731/1000, Meta Loss: 0.4862, Time: 0.61s\n",
      "Step 732/1000, Meta Loss: 0.5164, Time: 0.85s\n",
      "Step 733/1000, Meta Loss: 0.5328, Time: 0.74s\n",
      "Step 734/1000, Meta Loss: 0.5196, Time: 0.71s\n",
      "Step 735/1000, Meta Loss: 0.5164, Time: 0.80s\n",
      "Step 736/1000, Meta Loss: 0.4861, Time: 0.57s\n",
      "Step 737/1000, Meta Loss: 0.5458, Time: 0.57s\n",
      "Step 738/1000, Meta Loss: 0.5177, Time: 0.67s\n",
      "Step 739/1000, Meta Loss: 0.4804, Time: 0.61s\n",
      "Step 740/1000, Meta Loss: 0.5115, Time: 0.59s\n",
      "Step 741/1000, Meta Loss: 0.5112, Time: 0.70s\n",
      "Step 742/1000, Meta Loss: 0.5019, Time: 0.70s\n",
      "Step 743/1000, Meta Loss: 0.5005, Time: 0.74s\n",
      "Step 744/1000, Meta Loss: 0.5032, Time: 0.69s\n",
      "Step 745/1000, Meta Loss: 0.5246, Time: 0.79s\n",
      "Step 746/1000, Meta Loss: 0.5150, Time: 0.60s\n",
      "Step 747/1000, Meta Loss: 0.4960, Time: 0.76s\n",
      "Step 748/1000, Meta Loss: 0.5356, Time: 0.78s\n",
      "Step 749/1000, Meta Loss: 0.5018, Time: 0.86s\n",
      "Step 750/1000, Meta Loss: 0.5123, Time: 0.72s\n",
      "Step 751/1000, Meta Loss: 0.5074, Time: 0.62s\n",
      "Step 752/1000, Meta Loss: 0.5219, Time: 0.68s\n",
      "Step 753/1000, Meta Loss: 0.5355, Time: 0.78s\n",
      "Step 754/1000, Meta Loss: 0.4978, Time: 0.73s\n",
      "Step 755/1000, Meta Loss: 0.5596, Time: 0.77s\n",
      "Step 756/1000, Meta Loss: 0.4960, Time: 0.73s\n",
      "Step 757/1000, Meta Loss: 0.5214, Time: 0.72s\n",
      "Step 758/1000, Meta Loss: 0.5080, Time: 0.65s\n",
      "Step 759/1000, Meta Loss: 0.5046, Time: 0.61s\n",
      "Step 760/1000, Meta Loss: 0.4940, Time: 0.62s\n",
      "Step 761/1000, Meta Loss: 0.5081, Time: 0.59s\n",
      "Step 762/1000, Meta Loss: 0.4841, Time: 0.58s\n",
      "Step 763/1000, Meta Loss: 0.5058, Time: 0.63s\n",
      "Step 764/1000, Meta Loss: 0.5389, Time: 0.61s\n",
      "Step 765/1000, Meta Loss: 0.5335, Time: 0.59s\n",
      "Step 766/1000, Meta Loss: 0.4919, Time: 0.68s\n",
      "Step 767/1000, Meta Loss: 0.5097, Time: 0.63s\n",
      "Step 768/1000, Meta Loss: 0.4925, Time: 0.68s\n",
      "Step 769/1000, Meta Loss: 0.5357, Time: 0.72s\n",
      "Step 770/1000, Meta Loss: 0.5211, Time: 0.78s\n",
      "Step 771/1000, Meta Loss: 0.5587, Time: 0.76s\n",
      "Step 772/1000, Meta Loss: 0.5036, Time: 0.76s\n",
      "Step 773/1000, Meta Loss: 0.5044, Time: 0.64s\n",
      "Step 774/1000, Meta Loss: 0.5328, Time: 0.62s\n",
      "Step 775/1000, Meta Loss: 0.5193, Time: 0.62s\n",
      "Step 776/1000, Meta Loss: 0.4803, Time: 0.59s\n",
      "Step 777/1000, Meta Loss: 0.5471, Time: 0.59s\n",
      "Step 778/1000, Meta Loss: 0.5135, Time: 0.65s\n",
      "Step 779/1000, Meta Loss: 0.5080, Time: 0.59s\n",
      "Step 780/1000, Meta Loss: 0.5272, Time: 0.65s\n",
      "Step 781/1000, Meta Loss: 0.4962, Time: 0.58s\n",
      "Step 782/1000, Meta Loss: 0.5232, Time: 0.67s\n",
      "Step 783/1000, Meta Loss: 0.5043, Time: 0.60s\n",
      "Step 784/1000, Meta Loss: 0.5056, Time: 0.62s\n",
      "Step 785/1000, Meta Loss: 0.4894, Time: 0.75s\n",
      "Step 786/1000, Meta Loss: 0.5392, Time: 0.60s\n",
      "Step 787/1000, Meta Loss: 0.5228, Time: 0.70s\n",
      "Step 788/1000, Meta Loss: 0.5413, Time: 0.78s\n",
      "Step 789/1000, Meta Loss: 0.5302, Time: 0.82s\n",
      "Step 790/1000, Meta Loss: 0.5202, Time: 0.83s\n",
      "Step 791/1000, Meta Loss: 0.4944, Time: 0.59s\n",
      "Step 792/1000, Meta Loss: 0.5057, Time: 0.63s\n",
      "Step 793/1000, Meta Loss: 0.5123, Time: 0.81s\n",
      "Step 794/1000, Meta Loss: 0.5107, Time: 0.59s\n",
      "Step 795/1000, Meta Loss: 0.5353, Time: 0.81s\n",
      "Step 796/1000, Meta Loss: 0.5253, Time: 0.66s\n",
      "Step 797/1000, Meta Loss: 0.5383, Time: 0.77s\n",
      "Step 798/1000, Meta Loss: 0.5206, Time: 0.75s\n",
      "Step 799/1000, Meta Loss: 0.4957, Time: 0.73s\n",
      "Step 800/1000, Meta Loss: 0.5110, Time: 0.60s\n",
      "Step 801/1000, Meta Loss: 0.4842, Time: 0.60s\n",
      "Step 802/1000, Meta Loss: 0.5272, Time: 0.61s\n",
      "Step 803/1000, Meta Loss: 0.5163, Time: 0.60s\n",
      "Step 804/1000, Meta Loss: 0.5586, Time: 0.59s\n",
      "Step 805/1000, Meta Loss: 0.5689, Time: 0.61s\n",
      "Step 806/1000, Meta Loss: 0.4919, Time: 0.60s\n",
      "Step 807/1000, Meta Loss: 0.5201, Time: 0.80s\n",
      "Step 808/1000, Meta Loss: 0.5464, Time: 0.69s\n",
      "Step 809/1000, Meta Loss: 0.5507, Time: 0.60s\n",
      "Step 810/1000, Meta Loss: 0.4840, Time: 0.74s\n",
      "Step 811/1000, Meta Loss: 0.5147, Time: 0.67s\n",
      "Step 812/1000, Meta Loss: 0.5220, Time: 0.68s\n",
      "Step 813/1000, Meta Loss: 0.5262, Time: 0.61s\n",
      "Step 814/1000, Meta Loss: 0.5127, Time: 0.58s\n",
      "Step 815/1000, Meta Loss: 0.5102, Time: 0.62s\n",
      "Step 816/1000, Meta Loss: 0.5368, Time: 0.61s\n",
      "Step 817/1000, Meta Loss: 0.5211, Time: 0.57s\n",
      "Step 818/1000, Meta Loss: 0.5064, Time: 0.57s\n",
      "Step 819/1000, Meta Loss: 0.5354, Time: 0.56s\n",
      "Step 820/1000, Meta Loss: 0.4994, Time: 0.57s\n",
      "Step 821/1000, Meta Loss: 0.5347, Time: 0.60s\n",
      "Step 822/1000, Meta Loss: 0.4949, Time: 0.62s\n",
      "Step 823/1000, Meta Loss: 0.5251, Time: 0.62s\n",
      "Step 824/1000, Meta Loss: 0.5071, Time: 0.64s\n",
      "Step 825/1000, Meta Loss: 0.5062, Time: 0.67s\n",
      "Step 826/1000, Meta Loss: 0.5175, Time: 0.79s\n",
      "Step 827/1000, Meta Loss: 0.5464, Time: 0.86s\n",
      "Step 828/1000, Meta Loss: 0.4890, Time: 0.72s\n",
      "Step 829/1000, Meta Loss: 0.5110, Time: 0.62s\n",
      "Step 830/1000, Meta Loss: 0.5180, Time: 0.57s\n",
      "Step 831/1000, Meta Loss: 0.4898, Time: 0.57s\n",
      "Step 832/1000, Meta Loss: 0.5291, Time: 0.56s\n",
      "Step 833/1000, Meta Loss: 0.5312, Time: 0.56s\n",
      "Step 834/1000, Meta Loss: 0.5237, Time: 0.64s\n",
      "Step 835/1000, Meta Loss: 0.5376, Time: 0.63s\n",
      "Step 836/1000, Meta Loss: 0.5152, Time: 0.64s\n",
      "Step 837/1000, Meta Loss: 0.5233, Time: 0.60s\n",
      "Step 838/1000, Meta Loss: 0.5289, Time: 0.60s\n",
      "Step 839/1000, Meta Loss: 0.5195, Time: 0.65s\n",
      "Step 840/1000, Meta Loss: 0.5072, Time: 0.61s\n",
      "Step 841/1000, Meta Loss: 0.5133, Time: 0.62s\n",
      "Step 842/1000, Meta Loss: 0.5817, Time: 0.61s\n",
      "Step 843/1000, Meta Loss: 0.5113, Time: 0.59s\n",
      "Step 844/1000, Meta Loss: 0.5394, Time: 0.58s\n",
      "Step 845/1000, Meta Loss: 0.5308, Time: 0.59s\n",
      "Step 846/1000, Meta Loss: 0.4939, Time: 0.58s\n",
      "Step 847/1000, Meta Loss: 0.5608, Time: 0.57s\n",
      "Step 848/1000, Meta Loss: 0.5056, Time: 0.56s\n",
      "Step 849/1000, Meta Loss: 0.4900, Time: 0.65s\n",
      "Step 850/1000, Meta Loss: 0.5234, Time: 0.58s\n",
      "Step 851/1000, Meta Loss: 0.5350, Time: 0.57s\n",
      "Step 852/1000, Meta Loss: 0.4977, Time: 0.57s\n",
      "Step 853/1000, Meta Loss: 0.5151, Time: 0.57s\n",
      "Step 854/1000, Meta Loss: 0.5341, Time: 0.56s\n",
      "Step 855/1000, Meta Loss: 0.5195, Time: 0.57s\n",
      "Step 856/1000, Meta Loss: 0.5319, Time: 0.66s\n",
      "Step 857/1000, Meta Loss: 0.5286, Time: 0.61s\n",
      "Step 858/1000, Meta Loss: 0.5512, Time: 0.59s\n",
      "Step 859/1000, Meta Loss: 0.5099, Time: 0.57s\n",
      "Step 860/1000, Meta Loss: 0.5489, Time: 0.58s\n",
      "Step 861/1000, Meta Loss: 0.5008, Time: 0.62s\n",
      "Step 862/1000, Meta Loss: 0.4890, Time: 0.68s\n",
      "Step 863/1000, Meta Loss: 0.5142, Time: 0.78s\n",
      "Step 864/1000, Meta Loss: 0.5049, Time: 0.78s\n",
      "Step 865/1000, Meta Loss: 0.5526, Time: 0.61s\n",
      "Step 866/1000, Meta Loss: 0.5113, Time: 0.66s\n",
      "Step 867/1000, Meta Loss: 0.5109, Time: 0.58s\n",
      "Step 868/1000, Meta Loss: 0.4923, Time: 0.79s\n",
      "Step 869/1000, Meta Loss: 0.5055, Time: 0.75s\n",
      "Step 870/1000, Meta Loss: 0.5127, Time: 0.71s\n",
      "Step 871/1000, Meta Loss: 0.5253, Time: 0.75s\n",
      "Step 872/1000, Meta Loss: 0.5269, Time: 0.74s\n",
      "Step 873/1000, Meta Loss: 0.5166, Time: 0.67s\n",
      "Step 874/1000, Meta Loss: 0.5637, Time: 0.63s\n",
      "Step 875/1000, Meta Loss: 0.5317, Time: 0.63s\n",
      "Step 876/1000, Meta Loss: 0.5188, Time: 0.66s\n",
      "Step 877/1000, Meta Loss: 0.5428, Time: 0.62s\n",
      "Step 878/1000, Meta Loss: 0.4763, Time: 0.70s\n",
      "Step 879/1000, Meta Loss: 0.5344, Time: 0.60s\n",
      "Step 880/1000, Meta Loss: 0.4827, Time: 0.59s\n",
      "Step 881/1000, Meta Loss: 0.5241, Time: 0.66s\n",
      "Step 882/1000, Meta Loss: 0.4770, Time: 0.87s\n",
      "Step 883/1000, Meta Loss: 0.5151, Time: 1.01s\n",
      "Step 884/1000, Meta Loss: 0.5236, Time: 0.84s\n",
      "Step 885/1000, Meta Loss: 0.5213, Time: 0.62s\n",
      "Step 886/1000, Meta Loss: 0.5035, Time: 0.62s\n",
      "Step 887/1000, Meta Loss: 0.5205, Time: 0.58s\n",
      "Step 888/1000, Meta Loss: 0.5507, Time: 0.60s\n",
      "Step 889/1000, Meta Loss: 0.5127, Time: 0.85s\n",
      "Step 890/1000, Meta Loss: 0.5193, Time: 0.63s\n",
      "Step 891/1000, Meta Loss: 0.5636, Time: 0.60s\n",
      "Step 892/1000, Meta Loss: 0.5344, Time: 0.59s\n",
      "Step 893/1000, Meta Loss: 0.5605, Time: 0.58s\n",
      "Step 894/1000, Meta Loss: 0.5214, Time: 0.58s\n",
      "Step 895/1000, Meta Loss: 0.5098, Time: 0.57s\n",
      "Step 896/1000, Meta Loss: 0.5336, Time: 0.58s\n",
      "Step 897/1000, Meta Loss: 0.5473, Time: 0.58s\n",
      "Step 898/1000, Meta Loss: 0.5133, Time: 0.68s\n",
      "Step 899/1000, Meta Loss: 0.5160, Time: 0.59s\n",
      "Step 900/1000, Meta Loss: 0.4934, Time: 0.58s\n",
      "Step 901/1000, Meta Loss: 0.5120, Time: 0.59s\n",
      "Step 902/1000, Meta Loss: 0.5173, Time: 0.57s\n",
      "Step 903/1000, Meta Loss: 0.5481, Time: 0.57s\n",
      "Step 904/1000, Meta Loss: 0.4866, Time: 0.58s\n",
      "Step 905/1000, Meta Loss: 0.4999, Time: 0.57s\n",
      "Step 906/1000, Meta Loss: 0.4991, Time: 0.63s\n",
      "Step 907/1000, Meta Loss: 0.5170, Time: 0.59s\n",
      "Step 908/1000, Meta Loss: 0.4877, Time: 0.58s\n",
      "Step 909/1000, Meta Loss: 0.5413, Time: 0.55s\n",
      "Step 910/1000, Meta Loss: 0.5258, Time: 0.56s\n",
      "Step 911/1000, Meta Loss: 0.5208, Time: 0.56s\n",
      "Step 912/1000, Meta Loss: 0.5255, Time: 0.56s\n",
      "Step 913/1000, Meta Loss: 0.5214, Time: 0.57s\n",
      "Step 914/1000, Meta Loss: 0.4912, Time: 0.57s\n",
      "Step 915/1000, Meta Loss: 0.5150, Time: 0.57s\n",
      "Step 916/1000, Meta Loss: 0.5122, Time: 0.56s\n",
      "Step 917/1000, Meta Loss: 0.5065, Time: 0.59s\n",
      "Step 918/1000, Meta Loss: 0.5246, Time: 0.57s\n",
      "Step 919/1000, Meta Loss: 0.5658, Time: 0.57s\n",
      "Step 920/1000, Meta Loss: 0.5133, Time: 0.56s\n",
      "Step 921/1000, Meta Loss: 0.5060, Time: 0.64s\n",
      "Step 922/1000, Meta Loss: 0.5321, Time: 0.63s\n",
      "Step 923/1000, Meta Loss: 0.5578, Time: 0.66s\n",
      "Step 924/1000, Meta Loss: 0.5614, Time: 0.64s\n",
      "Step 925/1000, Meta Loss: 0.5201, Time: 0.58s\n",
      "Step 926/1000, Meta Loss: 0.5556, Time: 0.74s\n",
      "Step 927/1000, Meta Loss: 0.5367, Time: 0.71s\n",
      "Step 928/1000, Meta Loss: 0.5116, Time: 0.76s\n",
      "Step 929/1000, Meta Loss: 0.5215, Time: 0.70s\n",
      "Step 930/1000, Meta Loss: 0.5257, Time: 0.65s\n",
      "Step 931/1000, Meta Loss: 0.5018, Time: 0.61s\n",
      "Step 932/1000, Meta Loss: 0.5038, Time: 0.60s\n",
      "Step 933/1000, Meta Loss: 0.5291, Time: 0.62s\n",
      "Step 934/1000, Meta Loss: 0.5142, Time: 0.62s\n",
      "Step 935/1000, Meta Loss: 0.5186, Time: 0.61s\n",
      "Step 936/1000, Meta Loss: 0.5374, Time: 0.61s\n",
      "Step 937/1000, Meta Loss: 0.5237, Time: 0.59s\n",
      "Step 938/1000, Meta Loss: 0.5298, Time: 0.60s\n",
      "Step 939/1000, Meta Loss: 0.5330, Time: 0.61s\n",
      "Step 940/1000, Meta Loss: 0.5465, Time: 0.73s\n",
      "Step 941/1000, Meta Loss: 0.5163, Time: 0.73s\n",
      "Step 942/1000, Meta Loss: 0.4848, Time: 0.66s\n",
      "Step 943/1000, Meta Loss: 0.5293, Time: 0.60s\n",
      "Step 944/1000, Meta Loss: 0.5546, Time: 0.58s\n",
      "Step 945/1000, Meta Loss: 0.5006, Time: 0.61s\n",
      "Step 946/1000, Meta Loss: 0.5010, Time: 0.61s\n",
      "Step 947/1000, Meta Loss: 0.5224, Time: 0.57s\n",
      "Step 948/1000, Meta Loss: 0.5555, Time: 0.57s\n",
      "Step 949/1000, Meta Loss: 0.5039, Time: 0.57s\n",
      "Step 950/1000, Meta Loss: 0.5266, Time: 0.59s\n",
      "Step 951/1000, Meta Loss: 0.5620, Time: 0.58s\n",
      "Step 952/1000, Meta Loss: 0.5037, Time: 0.57s\n",
      "Step 953/1000, Meta Loss: 0.5052, Time: 0.57s\n",
      "Step 954/1000, Meta Loss: 0.5383, Time: 0.57s\n",
      "Step 955/1000, Meta Loss: 0.5493, Time: 0.67s\n",
      "Step 956/1000, Meta Loss: 0.5248, Time: 0.58s\n",
      "Step 957/1000, Meta Loss: 0.5292, Time: 0.57s\n",
      "Step 958/1000, Meta Loss: 0.5027, Time: 0.65s\n",
      "Step 959/1000, Meta Loss: 0.5298, Time: 0.58s\n",
      "Step 960/1000, Meta Loss: 0.5267, Time: 0.58s\n",
      "Step 961/1000, Meta Loss: 0.5027, Time: 0.58s\n",
      "Step 962/1000, Meta Loss: 0.5130, Time: 0.57s\n",
      "Step 963/1000, Meta Loss: 0.5189, Time: 0.57s\n",
      "Step 964/1000, Meta Loss: 0.5535, Time: 0.61s\n",
      "Step 965/1000, Meta Loss: 0.5161, Time: 0.57s\n",
      "Step 966/1000, Meta Loss: 0.4917, Time: 0.61s\n",
      "Step 967/1000, Meta Loss: 0.5197, Time: 0.57s\n",
      "Step 968/1000, Meta Loss: 0.5382, Time: 0.60s\n",
      "Step 969/1000, Meta Loss: 0.5303, Time: 0.58s\n",
      "Step 970/1000, Meta Loss: 0.5038, Time: 0.58s\n",
      "Step 971/1000, Meta Loss: 0.4991, Time: 0.65s\n",
      "Step 972/1000, Meta Loss: 0.5359, Time: 0.59s\n",
      "Step 973/1000, Meta Loss: 0.4828, Time: 0.57s\n",
      "Step 974/1000, Meta Loss: 0.4731, Time: 0.56s\n",
      "Step 975/1000, Meta Loss: 0.5075, Time: 0.57s\n",
      "Step 976/1000, Meta Loss: 0.5277, Time: 0.57s\n",
      "Step 977/1000, Meta Loss: 0.5096, Time: 0.56s\n",
      "Step 978/1000, Meta Loss: 0.4941, Time: 0.56s\n",
      "Step 979/1000, Meta Loss: 0.5215, Time: 0.58s\n",
      "Step 980/1000, Meta Loss: 0.5453, Time: 0.57s\n",
      "Step 981/1000, Meta Loss: 0.5054, Time: 0.57s\n",
      "Step 982/1000, Meta Loss: 0.5349, Time: 0.65s\n",
      "Step 983/1000, Meta Loss: 0.5384, Time: 0.61s\n",
      "Step 984/1000, Meta Loss: 0.5250, Time: 0.59s\n",
      "Step 985/1000, Meta Loss: 0.4962, Time: 0.58s\n",
      "Step 986/1000, Meta Loss: 0.5060, Time: 0.57s\n",
      "Step 987/1000, Meta Loss: 0.5528, Time: 0.57s\n",
      "Step 988/1000, Meta Loss: 0.5607, Time: 0.57s\n",
      "Step 989/1000, Meta Loss: 0.5501, Time: 0.57s\n",
      "Step 990/1000, Meta Loss: 0.5214, Time: 0.59s\n",
      "Step 991/1000, Meta Loss: 0.5153, Time: 0.57s\n",
      "Step 992/1000, Meta Loss: 0.5074, Time: 0.63s\n",
      "Step 993/1000, Meta Loss: 0.5261, Time: 0.58s\n",
      "Step 994/1000, Meta Loss: 0.5491, Time: 0.57s\n",
      "Step 995/1000, Meta Loss: 0.5397, Time: 0.56s\n",
      "Step 996/1000, Meta Loss: 0.5374, Time: 0.57s\n",
      "Step 997/1000, Meta Loss: 0.5299, Time: 0.57s\n",
      "Step 998/1000, Meta Loss: 0.4928, Time: 0.57s\n",
      "Step 999/1000, Meta Loss: 0.5099, Time: 0.56s\n",
      "Step 1000/1000, Meta Loss: 0.5157, Time: 0.60s\n",
      "FOMAML training done\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import trange\n",
    "import time\n",
    "\n",
    "print(\"Starting the FOMAML Training...\")\n",
    "for step in trange(training_step, desc=\"Meta-training steps\"):\n",
    "    start_time = time.time()\n",
    "    support_x, support_y, query_x, query_y = sample_task_batch(\n",
    "        train_data_byclass, batch_size=meta_batch_size, n_way=n_way, k_shot=k_shot, q_queries=q_queries)\n",
    "    meta_optimizer.zero_grad()\n",
    "    total_meta_ls = 0.0\n",
    "    for i in range(meta_batch_size):\n",
    "        fast_model = copy.deepcopy(meta_model).to(device)\n",
    "        fast_optim = optim.SGD(fast_model.parameters(), lr=inner_lr)\n",
    "        sx, sy = support_x[i].to(device), support_y[i].to(device)\n",
    "        qx, qy = query_x[i].to(device), query_y[i].to(device)\n",
    "        for _ in range(inner_steps):\n",
    "            fast_optim.zero_grad()\n",
    "            logits = fast_model(sx)\n",
    "            loss = F.cross_entropy(logits, sy)\n",
    "            loss.backward()\n",
    "            fast_optim.step()\n",
    "        query_logits = fast_model(qx)\n",
    "        task_meta_loss = F.cross_entropy(query_logits, qy)\n",
    "        total_meta_ls += task_meta_loss\n",
    "    average_meta_ls = total_meta_ls / meta_batch_size\n",
    "    average_meta_ls.backward()\n",
    "    meta_optimizer.step()\n",
    "    if (step + 1) % 1 == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"Step {step+1}/{training_step}, Meta Loss: {average_meta_ls.item():.4f}, Time: {elapsed:.2f}s\")\n",
    "print(\"FOMAML training done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a00f969d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after 0 adaptation steps: 0.1287\n",
      "Accuracy after 1 adaptation steps: 0.1761\n",
      "Accuracy after 2 adaptation steps: 0.2442\n",
      "Accuracy after 3 adaptation steps: 0.2995\n",
      "Accuracy after 4 adaptation steps: 0.3519\n",
      "Accuracy after 5 adaptation steps: 0.3856\n",
      "Accuracy after 6 adaptation steps: 0.4021\n",
      "Accuracy after 7 adaptation steps: 0.4121\n",
      "Accuracy after 8 adaptation steps: 0.4176\n",
      "Accuracy after 9 adaptation steps: 0.4217\n",
      "Accuracy after 10 adaptation steps: 0.4249\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_maml(meta_model, data_by_class, n_tasks, n_way, k_shot, q_queries, inner_steps):\n",
    "    \"\"\"\n",
    "    Evaluates the MAML model on a set of new tasks.\n",
    "    \"\"\"\n",
    "    meta_model.eval() # Set the model to evaluation mode\n",
    "    total_accuracies = [0.0] * (inner_steps + 1)\n",
    "\n",
    "    for _ in range(n_tasks):\n",
    "        # Sample a new task\n",
    "        support_x, support_y, query_x, query_y = sample_task_batch(\n",
    "            data_by_class, 1, n_way, k_shot, q_queries\n",
    "        )\n",
    "        # Remove the batch dimension of 1\n",
    "        support_x, support_y = support_x[0].to(device), support_y[0].to(device)\n",
    "        query_x, query_y = query_x[0].to(device), query_y[0].to(device)\n",
    "\n",
    "        # 0. Evaluate before adaptation (0-shot)\n",
    "        with torch.no_grad():\n",
    "            query_logits = meta_model(query_x)\n",
    "            preds = torch.argmax(query_logits, dim=1)\n",
    "            accuracy = (preds == query_y).float().mean().item()\n",
    "            total_accuracies[0] += accuracy\n",
    "\n",
    "        # Create a fast model for adaptation\n",
    "        fast_model = copy.deepcopy(meta_model)\n",
    "        fast_optimizer = optim.SGD(fast_model.parameters(), lr=inner_lr)\n",
    "\n",
    "        # 1. Adapt and evaluate for each inner step\n",
    "        for step in range(inner_steps):\n",
    "            # Adapt on support set\n",
    "            fast_optimizer.zero_grad()\n",
    "            logits = fast_model(support_x)\n",
    "            loss = F.cross_entropy(logits, support_y)\n",
    "            loss.backward()\n",
    "            fast_optimizer.step()\n",
    "\n",
    "            # Evaluate on query set\n",
    "            with torch.no_grad():\n",
    "                query_logits = fast_model(query_x)\n",
    "                preds = torch.argmax(query_logits, dim=1)\n",
    "                accuracy = (preds == query_y).float().mean().item()\n",
    "                total_accuracies[step + 1] += accuracy\n",
    "\n",
    "    # Calculate average accuracies\n",
    "    avg_accuracies = [acc / n_tasks for acc in total_accuracies]\n",
    "    return avg_accuracies\n",
    "\n",
    "# --- Load Test Data ---\n",
    "mnist_test = datasets.MNIST('data', train=False, download=True, transform=transform)\n",
    "test_data_by_class = {i: [] for i in range(10)}\n",
    "for x, y in mnist_test:\n",
    "    test_data_by_class[y].append(x)\n",
    "for i in range(10):\n",
    "    test_data_by_class[i] = torch.stack(test_data_by_class[i])\n",
    "\n",
    "# --- Run Evaluation ---\n",
    "N_EVAL_TASKS = 100\n",
    "ADAPTATION_STEPS = 10\n",
    "eval_accuracies = evaluate_maml(\n",
    "    meta_model, test_data_by_class, N_EVAL_TASKS, n_way=5, k_shot=5, q_queries=16, inner_steps=10\n",
    ")\n",
    "\n",
    "for i, acc in enumerate(eval_accuracies):\n",
    "    print(f\"Accuracy after {i} adaptation steps: {acc:.4f}\")\n",
    "\n",
    "# --- Plotting ---\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(f\"{n_way}-Way {k_shot}-Shot Accuracy on Test Tasks\")\n",
    "plt.xlabel(\"Number of Adaptation Steps\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.bar(range(ADAPTATION_STEPS + 1), eval_accuracies, color='skyblue')\n",
    "plt.xticks(range(ADAPTATION_STEPS + 1))\n",
    "plt.ylim([0, 1])\n",
    "for i, acc in enumerate(eval_accuracies):\n",
    "    plt.text(i, acc + 0.02, f\"{acc:.3f}\", ha='center', color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da53951",
   "metadata": {},
   "source": [
    "- This vanilla implementation of MAML on MNIST successfully demonstrates the core concept of `\"learning to learn.\"` \n",
    "- The evaluation shows that the *model's accuracy consistently improves* with each inner-loop adaptation step on new, unseen tasks, proving that the meta-training process has found an initial set of weights that is amenable to fast fine-tuning. However, the final accuracy is only marginally better than random guessing. \n",
    "- This is not a limitation of the simple CNN architecture but rather **highlights that this is a conceptual proof-of-concept.** \n",
    "-Achieving high performance with MAML is notoriously sensitive to hyperparameters, particularly the inner and meta learning rates, and requires a significantly longer training regime to converge to a strong solution. \n",
    "\n",
    "The results here confirm the mechanism works, while underscoring that extensive training and tuning are necessary to move from a conceptual model to a powerful few-shot classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f676bd87",
   "metadata": {},
   "source": [
    "## Actual MAML : with second order derivatives (calculated via Hessian vector products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5985a5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Outer Lopp: Meta- training --\n",
    "for step in range(training_step):\n",
    "    # we perform the sampling\n",
    "    support_x, support_y, query_x, query_y = sample_task_batch(train_data_byclass, batch_size=meta_batch_size, n_way=n_way, k_shot=k_shot, q_queries=q_queries)\n",
    "    total_meta_ls = 0.0\n",
    "    meta_optimizer.zero_grad()\n",
    "\n",
    "    # Process each task in the bact\n",
    "    for i in range(meta_batch_size):\n",
    "        sx, sy = support_x[i].to(device), support_y[i].to(device)\n",
    "        qx, qy = query_x[i].to(device), query_y[i].to(device)\n",
    "\n",
    "        # -- How actually we take the gradient thru a gradient\n",
    "        # Start with the original meta-model weight for each tasks\n",
    "        fast_weight = list(meta_model.parameters())\n",
    "\n",
    "        # 1. Inner loop: task specific weight adaptations\n",
    "        logits = F.conv2d(sx, fast_weight[0], fast_weight[1], stride=1, padding=1)\n",
    "        logits = F.batch_norm(logits, running_mean=None, running_var=None, weight=fast_weight[2], bias=fast_weight[3], training=True)\n",
    "        logits = F.relu(logits)\n",
    "        logits = F.max_pool2d(logits, 2)\n",
    "        logits = F.conv2d(logits, fast_weight[4], fast_weight[5], stride=1, padding=1)\n",
    "        logits = F.batch_norm(logits, running_mean=None, running_var= None, weight=fast_weight[6], bias= fast_weight[7], training=True)\n",
    "        logits = F.relu(logits)\n",
    "        logits = F.max_pool2d(logits, 2)\n",
    "        logits = logits.view(logits.size(0), -1)\n",
    "        logits = F.linear(logits, fast_weight[8], fast_weight[9])\n",
    "\n",
    "        loss = F.cross_entropy(logits, sy)\n",
    "        # Calculate the gradients wrt fast_weight and store the graph (for 2nd order derivative)\n",
    "        grad = torch.autograd.grad(loss, fast_weight, create_graph=True)\n",
    "        # Weight update\n",
    "        fast_weight = [w - inner_lr*g for w,g in zip(fast_weight, grad)]\n",
    "\n",
    "    # 2. calculate the meta loss based on the query set\n",
    "    # perform another full forward pass as above"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95c3c4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "import random\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "\n",
    "# -- To insert non functional code in the image to get augmented image --\n",
    "class CodeInserter:\n",
    "    def __init__(self, image: Image.Image):\n",
    "        self.image = image\n",
    "        self.flat_img = np.array(image).ravel()\n",
    "        self.opcodes = [0, 144, 204, 205]\n",
    "        self.patterns = [\n",
    "            [0],              # Simulates null padding\n",
    "            [144],            # Simulates single-byte NOP\n",
    "            [204],            # Simulates debug breakpoint\n",
    "            #[255],            # Simulates filler bytes\n",
    "            [102, 144]        # Simulates 2-byte NOP (0x66 0x90)\n",
    "        ]\n",
    "    def _generate_random_slice(self, min_len=40, max_len=400):\n",
    "        \"\"\"\n",
    "        Generates a slice of noise using one of several realistic strategies.\n",
    "        \"\"\"\n",
    "        slice_length = random.randint(min_len, max_len)\n",
    "        # Randomly choose a generation mode for this slice\n",
    "        generation_mode = random.choice(['pattern', 'structured_noise'])\n",
    "        if generation_mode == 'pattern':\n",
    "            # --- Strategy 1: Repeat a specific, ordered pattern ---\n",
    "            chosen_pattern = random.choice(self.patterns)\n",
    "            num_repeats = (slice_length // len(chosen_pattern)) + 1# Tile the pattern until it's long enough\n",
    "            tiled_pattern = np.tile(chosen_pattern, num_repeats)\n",
    "            return tiled_pattern[:slice_length]# Trim to the exact slice_length\n",
    "        else:\n",
    "            # --- Strategy 2: Create a random mix of opcodes ---\n",
    "            num_opcodes = random.randint(2, len(self.opcodes))\n",
    "            chosen_opcodes = random.sample(self.opcodes, num_opcodes)\n",
    "            return np.random.choice(chosen_opcodes, size=slice_length)\n",
    "    def _calculate_new_dimensions(self, new_pixel_count: int):\n",
    "        \"\"\"\n",
    "        Calculates new width and height, preserving aspect ratio.\n",
    "        This is a simpler, more direct way to do the math.\n",
    "        \"\"\"\n",
    "        og_w, og_h = self.image.size\n",
    "        aspect_ratio = og_w / og_h\n",
    "        new_h = int(np.sqrt(new_pixel_count / aspect_ratio))\n",
    "        new_w = int(new_h * aspect_ratio)\n",
    "        # Pad to ensure the new canvas is big enough\n",
    "        while new_w * new_h < new_pixel_count:\n",
    "            new_w += 1\n",
    "        return new_w, new_h\n",
    "    def augment(self):\n",
    "        \"\"\"\n",
    "        Applies the insertion augmentation and returns a new PIL Image.\n",
    "        \"\"\"\n",
    "        augmented_image = self.flat_img.copy()\n",
    "        num_insertions = random.randint(1, 3)\n",
    "        max_idx = len(augmented_image)\n",
    "        cut_points = np.random.choice(max_idx, size=num_insertions, replace=False)\n",
    "        cut_points.sort()\n",
    "        for point in reversed(cut_points):\n",
    "            insertion_slice = self._generate_random_slice()\n",
    "            augmented_image = np.insert(augmented_image, point, insertion_slice)\n",
    "        new_w, new_h = self._calculate_new_dimensions(len(augmented_image))\n",
    "        pad_len = (new_w * new_h) - len(augmented_image)# pad the array to fit the new rectangular shape\n",
    "        padded_image = np.pad(augmented_image, (0, pad_len), 'constant')\n",
    "        reshaped_image = padded_image.reshape((new_h, new_w)).astype(np.uint8)\n",
    "        final_image = Image.fromarray(reshaped_image) # construct PIL image\n",
    "        #final_image.resize((64, 64), Image.Resampling.LANCZOS)\n",
    "        return final_image.resize((64, 64), Image.Resampling.LANCZOS)\n",
    "\n",
    "# -- To duplicate functional code in the image to get augmented image --\n",
    "class CodeDuplicator:\n",
    "    def __init__(self, image: Image.Image):\n",
    "        self.image = image\n",
    "        self.flat_img = np.array(image).ravel()\n",
    "        self.img_height, self.img_width = image.height, image.width\n",
    "\n",
    "    def _get_duplication_parameters(self):\n",
    "        # Using a square patch with a side length between 8 and 20 pixels.\n",
    "        side_length = random.randint(8, 20)\n",
    "        #window = side_length * side_length\n",
    "        window =int(0.1 * self.flat_img.size)  # 10 % of the input image size\n",
    "\n",
    "        max_start = len(self.flat_img) - window\n",
    "        start_point = random.randint(0, max_start)\n",
    "        while True:\n",
    "            insertion_point = random.randint(0, max_start)\n",
    "            if abs(start_point - insertion_point) >= window:\n",
    "                break # found valid\n",
    "        return start_point, insertion_point, window\n",
    "\n",
    "    def augment(self):\n",
    "        \"\"\"\n",
    "        Applies the duplication augmentation and returns a new PIL Image.\n",
    "        This is the main public method to call.\n",
    "        \"\"\"\n",
    "        start, duplicate_at, window = self._get_duplication_parameters()\n",
    "        augmented_flat = self.flat_img.copy()\n",
    "        snippet = self.flat_img[start : start + window]\n",
    "        augmented_flat[duplicate_at : duplicate_at + window] = snippet\n",
    "        augmented_2d = np.reshape(augmented_flat, (self.img_height, self.img_width))\n",
    "        return Image.fromarray(augmented_2d.astype(np.uint8))\n",
    "\n",
    "# -- Sampling and creation of batches --\n",
    "class TaskSampler:\n",
    "    def __init__(self, resized_folder: str, families: list, meta_batch_size: int, n_way: int, k_shot: int, q_query: int):\n",
    "        self.resized_folder = resized_folder\n",
    "        self.families = families\n",
    "        self.meta_batch_size = meta_batch_size\n",
    "        self.n_way = n_way\n",
    "        self.k_shot = k_shot\n",
    "        self.q_query = q_query\n",
    "        # Pre-load all image paths for efficiency\n",
    "        self.image_paths_by_family = {\n",
    "            f: [os.path.join(resized_folder, f, img_name) for img_name in os.listdir(os.path.join(resized_folder, f))]\n",
    "            for f in self.families\n",
    "        }\n",
    "        # Define a single transform to convert PIL Images to PyTorch Tensors\n",
    "        self.to_tensor = transforms.ToTensor() # output on using on one image is (1, H, W)\n",
    "    def _create_input_tensor(self, base_img: Image.Image) -> torch.Tensor:\n",
    "        \"\"\"Creates the 3-channel tensor from a single PIL image.\"\"\"\n",
    "        # Instantiate your augmentation classes for each image\n",
    "        aug1_img = CodeInserter(base_img).augment()\n",
    "        aug2_img = CodeDuplicator(base_img).augment()\n",
    "        base_tensor = self.to_tensor(base_img)\n",
    "        aug1_tensor = self.to_tensor(aug1_img)\n",
    "        aug2_tensor = self.to_tensor(aug2_img)\n",
    "        # Note: to_tensor creates a (1, H, W) tensor, so we cat on dim 0\n",
    "        return torch.cat([base_tensor, aug1_tensor, aug2_tensor], dim=0)\n",
    "    def sample(self):\n",
    "        \"\"\"\n",
    "        Samples a full meta-batch of tasks.\n",
    "        Returns:\n",
    "            A tuple of (support_x, support_y, query_x, query_y) tensors.\n",
    "        \"\"\"\n",
    "        support_x_batch, support_y_batch = [], []\n",
    "        query_x_batch, query_y_batch = [], []\n",
    "\n",
    "        for _ in range(self.meta_batch_size):\n",
    "            support_x, support_y = [], []\n",
    "            query_x, query_y = [], []\n",
    "            task_families = random.sample(self.families, self.n_way) # has the names of the families present in this task\n",
    "\n",
    "            for i, family in enumerate(task_families):\n",
    "                all_paths = self.image_paths_by_family[family] # has all possible img path for specific family\n",
    "                sampled_paths = random.sample(all_paths, self.k_shot + self.q_query)\n",
    "                # Load images, augment, and convert to tensors\n",
    "                class_tensors = [self._create_input_tensor(Image.open(p)) for p in sampled_paths]\n",
    "                class_tensors = torch.stack(class_tensors) # Shape: (k+q, 3, 64, 64)\n",
    "\n",
    "                # Split into support and query sets\n",
    "                support_set = class_tensors[:self.k_shot]\n",
    "                query_set = class_tensors[self.k_shot:]\n",
    "                support_x.append(support_set)\n",
    "                query_x.append(query_set)\n",
    "                # Create labels\n",
    "                support_y.append(torch.full((self.k_shot,), i, dtype=torch.long))\n",
    "                query_y.append(torch.full((self.q_query,), i, dtype=torch.long))\n",
    "\n",
    "            # Aggregate all classes for this one task\n",
    "            support_x_batch.append(torch.cat(support_x, dim=0))\n",
    "            support_y_batch.append(torch.cat(support_y, dim=0))\n",
    "            query_x_batch.append(torch.cat(query_x, dim=0))\n",
    "            query_y_batch.append(torch.cat(query_y, dim=0))\n",
    "\n",
    "        # Stack all tasks to create the final meta-batch\n",
    "        return (torch.stack(support_x_batch), torch.stack(support_y_batch),\n",
    "                torch.stack(query_x_batch), torch.stack(query_y_batch))\n",
    "\n",
    "# -- Hyper-parameters for the model --\n",
    "@dataclass\n",
    "class Hyperparameters:\n",
    "    n_way : int # total number of classes we want to classify in\n",
    "    k_shot : int # support examples in each task\n",
    "    q_query : int # query examples for each task\n",
    "    inner_lr : float # inner loop (task specific) learning rate (alpha in the original paper)\n",
    "    meta_lr : float # meta-learning rate (beta in the original paper)\n",
    "    meta_batch_size : int # total number of tasks per meta-batch\n",
    "    traning_steps : int # number of meta-updates to perform\n",
    "    inner_steps : int # number of gradient steps ('n': from the original paper)\n",
    "    meta_training_families : List # the list of tasks (families) from which we sample\n",
    "    meta_testing_families : List # the list of tasks (families) which are unknown and used for testing\n",
    "    meta_val_families : List # the list of tasks (families) for validation\n",
    "\n",
    "# -- Handle the updates in Learning Rate --\n",
    "class LearningRate:\n",
    "    def __init__(self, initial_inner_lr: int):\n",
    "        self.alpha1 = initial_inner_lr\n",
    "    def _upate_alpha(self, inner_lr: int, inner_steps: int, task_num: int):\n",
    "        alpha_i = inner_lr\n",
    "        n = inner_steps\n",
    "        delta1 = self.alpha1/4\n",
    "        delta2 = self.alpha1 / (n +1)\n",
    "        if 1<= task_num < n +1: return alpha_i + delta1\n",
    "        if n+1<=task_num <2*n+1: return alpha_i - delta1\n",
    "        if 2*n+1<=task_num<3*n+1: return alpha_i + delta2\n",
    "        if 3*n+1<=task_num<4*n+1: return alpha_i - delta2\n",
    "    def _update_beta(self, meta_lr: int, inner_steps: int):\n",
    "        return meta_lr / inner_steps\n",
    "\n",
    "# -- Handles the accurate update of both learning rates --\n",
    "class LearningRateManager:\n",
    "    def __init__(self, initial_lr, meta_optimizer, inner_steps):\n",
    "        self.inner_lr = initial_lr\n",
    "        self.initial_lr = initial_lr\n",
    "        self.meta_optimizer = meta_optimizer\n",
    "        self.inner_steps = inner_steps\n",
    "        self.val_loss_plateau_counter = 0\n",
    "    def _get_inner_lr(self, task_idx):\n",
    "        # Implements the DILLR\n",
    "        period = 4*self.inner_steps\n",
    "        n= self.inner_steps\n",
    "        i = task_idx % period\n",
    "        delta1 = self.initial_lr/4\n",
    "        delta2 = self.initial_lr/(n+1)\n",
    "        if 0<=i<n: self.inner_lr+=delta1\n",
    "        if n<=i<2*n: self.inner_lr-=delta1\n",
    "        if 2*n<=i<3*n: self.inner_lr+=delta2\n",
    "        if 3*n<=i<4*n: self.inner_lr-=delta2\n",
    "        return self.inner_lr\n",
    "    def _check_plateau(self, current_val_loss, best_val_loss):\n",
    "        # Implements the AOLLR for the meta_lr\n",
    "        if current_val_loss >= best_val_loss:\n",
    "            self.val_loss_plateau_counter+=1\n",
    "        else:\n",
    "            self.val_loss_plateau_counter = 0 # reset as better validation performance\n",
    "        if self.val_loss_plateau_counter >= self.inner_steps:\n",
    "            print(f'Validation loss has plateaued. Reducing meta_lr')\n",
    "            for g in self.meta_optimizer.param_groups:\n",
    "                g['lr'] *= (1 / self.inner_steps)\n",
    "            self.val_loss_plateau_counter = 0 # reset after reducing\n",
    "\n",
    "# -- Base Learner Model --\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, in_channel: int, n_way: int):\n",
    "        super(CNN, self).__init__()\n",
    "        # Define the 4 CNN blocks\n",
    "        self.layer1 = self._make_conv_block(in_channel, 64)\n",
    "        self.layer2 = self._make_conv_block(64,64)\n",
    "        self.layer3 = self._make_conv_block(64,64)\n",
    "        self.layer4 = self._make_conv_block(64,64)\n",
    "        self.dropout = nn.Dropout(0.25),\n",
    "        self.classifier = nn.Linear(64*4*4, n_way)\n",
    "    def _make_conv_block(self, in_channels: int, out_channels: int):\n",
    "        \"\"\" Helper function to make convolutional block\n",
    "        Parameters\n",
    "        ----------\n",
    "        in_channels : int\n",
    "            the input channels for the conv layer\n",
    "        out_channels : int\n",
    "            the output channel of the layer (total number of filters applied)\n",
    "        \"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
    "        # First Group\n",
    "        x = self.layer1(x)\n",
    "        x= self.layer2(x)\n",
    "        x = self.dropout(x)\n",
    "        # Second Group\n",
    "        x = self.laye3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.dropout(x)\n",
    "        # Flatten the output for the linear layer\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # Final logits\n",
    "        logits = self.classifier(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bcd0929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "print(f'Using: {device}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d2bc6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyper parameters\n",
    "hp = Hyperparameters(\n",
    "    n_way=5,\n",
    "    k_shot=2,\n",
    "    q_query=15,\n",
    "    inner_lr=0.01,\n",
    "    meta_lr=0.001,\n",
    "    meta_batch_size=16,\n",
    "    traning_steps=100*100,\n",
    "    inner_steps=5,\n",
    "    meta_training_families=['Adialer.C', 'Agent.FYI', 'Allaple.A', 'Alueron.gen!J', 'Autorun.K', 'C2LOP.gen!g', 'Lolyda.AA2', 'Lolyda.AT',\n",
    "                        'Rbot!gen', 'Skintrim.N', 'Swizzor.gen!E', 'Swizzor.gen!I', 'VB.AT', 'Wintrim.BX', 'Yuner.A'],\n",
    "    meta_val_families=['Dontovo.A', 'Fakerean', 'Instantaccess', 'Lolyda.AA3', 'Obfuscator.AD'],\n",
    "    meta_testing_families=['Dialplatform.B', 'Allaple.L', 'C2LOP.P', 'Lolyda.AA1', 'Malex.gen!J']\n",
    ")\n",
    "# Set Task Sampler\n",
    "sampler = TaskSampler(\n",
    "    resized_folder='malimg_resized',\n",
    "    families=hp.meta_training_families,\n",
    "    meta_batch_size=hp.meta_batch_size,\n",
    "    n_way=hp.n_way,\n",
    "    k_shot=hp.k_shot,\n",
    "    q_query=hp.q_query\n",
    ")\n",
    "# Initialize the model and set it to device\n",
    "meta_model = CNN(in_channel=3, n_way=hp.n_way).to(device=device)\n",
    "meta_optimizer = optim.Adam(meta_model.parameters(), lr=hp.meta_lr)\n",
    "# Set Learning Rate\n",
    "lr_manager = LearningRateManager(initial_lr=hp.inner_lr, meta_optimizer=meta_optimizer, inner_steps=hp.inner_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b7effc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the model\n",
    "def evaluate(model, families, n_way, k_shot, q_query, num_eval_tasks=100):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    # temp sampler for evaluation\n",
    "    eval_sampler = TaskSampler(resized_folder='malimg_resized', families=families,\n",
    "                               meta_batch_size=num_eval_tasks, n_way=n_way, k_shot=k_shot, q_query=q_query)\n",
    "    support_x, support_y, query_x, query_y = eval_sampler.sample()\n",
    "    support_x, support_y = support_x.to(device=device), support_y.to(device=device)\n",
    "    query_x, query_y = query_x.to(device=device), query_y.to(device=device)\n",
    "\n",
    "    for i in range(num_eval_tasks):\n",
    "        with torch.no_grad():\n",
    "            fast_model = copy.deepcopy(model)\n",
    "            sx, sy = support_x[i], support_y[i]\n",
    "            qx, qy = query_x[i], query_y[i]\n",
    "\n",
    "            # Adapt model on the support (no optimizer needed)\n",
    "            for _ in range(hp.inner_steps):\n",
    "                logits = fast_model(sx)\n",
    "                loss = F.cross_entropy(logits, sy)\n",
    "                grads = torch.autograd.grad(loss, fast_model.parameters())\n",
    "                # Update the inner loop\n",
    "                for p, g in zip(fast_model.parameters(), grads):\n",
    "                    p.data -= hp.inner_lr*g\n",
    "                # Evaluate on query\n",
    "                query_logits = fast_model(qx)\n",
    "                preds = torch.argmax(query_logits, dim=1)\n",
    "                total_correct+= (preds==qy).sum().item()\n",
    "                total_samples+= len(qy)\n",
    "        return total_correct / total_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d12b978",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Logging and Chekpoint variables\n",
    "best_val_acc = 0.0\n",
    "steps_per_epoch = 100\n",
    "print(f'Starting thr Mi-MAML training on : {device}')\n",
    "for step in tqdm(range(hp.traning_steps), desc=\"Mi-MAML training\"):\n",
    "    meta_model.train()\n",
    "    support_x, support_y, query_x, query_y = sampler.sample()\n",
    "    support_x, support_y = support_x.to(device=device), support_y.to(device=device)\n",
    "    query_x, query_y = query_x.to(device=device), query_y.to(device=device)\n",
    "\n",
    "    meta_optimizer.zero_grad()\n",
    "    total_query_loss = 0.0\n",
    "\n",
    "    for i in range(hp.meta_batch_size):\n",
    "        fast_model = copy.deepcopy(meta_model)\n",
    "        sx, sy = support_x[i], support_y[i]\n",
    "        qx, qy = query_x[i], query_y[i]\n",
    "        # Inner loop adaptation FOMAML\n",
    "        for j in range(hp.inner_steps):\n",
    "            logits = fast_model(sx)\n",
    "            loss = F.cross_entropy(logits, sy)\n",
    "            grads = torch.autograd.grad(loss, fast_model.parameters(), create_graph=False)\n",
    "            # Update the fast_model parameters\n",
    "            current_inner_lr = lr_manager._get_inner_lr(step*hp.meta_batch_size+i)\n",
    "            for p, g in zip(fast_model.parameters(), grads):\n",
    "                p.data -= current_inner_lr*g\n",
    "        # Calculate the loss on query\n",
    "        query_logits = fast_model(qx)\n",
    "        query_loss = F.cross_entropy(query_logits, qy)\n",
    "        total_query_loss+= query_loss\n",
    "    # Average the loss across all the tasks and do meta-update\n",
    "    average_meta_ls = total_query_loss / hp.meta_batch_size\n",
    "    average_meta_ls.backward()\n",
    "    meta_optimizer.step()\n",
    "\n",
    "    # Validation and checkpointing\n",
    "    if (step+1) % steps_per_epoch == 0:\n",
    "        epoch = (step + 1) // steps_per_epoch\n",
    "        val_acc = evaluate (meta_model, hp.meta_val_families, hp.n_way, hp.k_shot, hp.q_query)\n",
    "        print(f\"Epoch {epoch}/{hp.traning_steps // steps_per_epoch} | Meta Loss: {average_meta_ls.item():.4f} | Val Acc: {val_acc:.4f}\")\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            print(f\"  -> New best validation accuracy! Saving model...\")\n",
    "            torch.save(meta_model.state_dict(), \"best_maml_model.pth\")\n",
    "        # The paper doesn't mention using validation loss for AOLLR, but it's a common practice.\n",
    "        # lr_manager.check_plateau(current_val_loss, best_val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42114121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MISSING PIECE: Final Testing ---\n",
    "print(\"\\nTraining finished. Loading best model and evaluating on the test set.\")\n",
    "meta_model.load_state_dict(torch.load(\"best_maml_model.pth\"))\n",
    "test_accuracy = evaluate(meta_model, hp.meta_testing_families, hp.n_way, hp.k_shot, hp.q_query)\n",
    "print(f\"\\nFinal Test Accuracy: {test_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
